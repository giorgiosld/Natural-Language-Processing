{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/giorgiosld/Natural-Language-Processing/blob/main/lab2/T_725_Lab02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pajD7caf9pX7"
      },
      "source": [
        "# T-725 Natural Language Processing: Lab 2\n",
        "In today's lab, we will be working with text classification.\n",
        "\n",
        "To begin with, do the following:\n",
        "* Select `\"File\" > \"Save a copy in Drive\"` to create a local copy of this notebook that you can edit.\n",
        "* Select `\"Runtime\" > \"Run all\"` to run the code in this notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsSxs1XKhi_4"
      },
      "source": [
        "## List comprehensions in Python\n",
        "List comprehensions are a concise way of creating lists in Python, and take the form:\n",
        "\n",
        "```python\n",
        "[expression for item in iterable]\n",
        "```\n",
        "\n",
        "A list comprehension creates a new list by evaluating some expression for every item in a given iterable (such as a string, a list or a dictionary). Let's look at an example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxB1Ip77hzFL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb21928e-0b20-4154-c3dd-bd369ae8038b"
      },
      "source": [
        "sentence = \"In a hole in the ground there lived a hobbit.\"\n",
        "words = sentence.split()\n",
        "print(words)\n",
        "\n",
        "# Example of a list comprehension\n",
        "word_lengths = [len(word) for word in words]\n",
        "print(word_lengths)\n",
        "\n",
        "# This is equal to\n",
        "word_lengths = []\n",
        "for word in words:\n",
        "  word_lengths.append(len(word))\n",
        "\n",
        "print(word_lengths)"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['In', 'a', 'hole', 'in', 'the', 'ground', 'there', 'lived', 'a', 'hobbit.']\n",
            "[2, 1, 4, 2, 3, 6, 5, 5, 1, 7]\n",
            "[2, 1, 4, 2, 3, 6, 5, 5, 1, 7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWw6IRBgkQbc"
      },
      "source": [
        "You can also add a conditional statement to list comprehensions, so that the expression will only be evaluated for items that meet a certain criteria:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSRG8kBdjZKA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "925096e7-c646-4ceb-f392-358c37665c70"
      },
      "source": [
        "e_words = [word for word in words if len(word) > 5]\n",
        "print(e_words)"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ground', 'hobbit.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtTTJDJ5qZML"
      },
      "source": [
        "Python also has set and dictionary comprehensions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jniqHWPoqd8E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad876751-a8d8-4a06-c873-0fa3ad38bfa5"
      },
      "source": [
        "lowercase_characters = {c.lower() for c in sentence}\n",
        "print(lowercase_characters)\n",
        "\n",
        "word_length = {word: len(word) for word in words}\n",
        "print(word_length['ground'])"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'a', 'g', 'b', 'n', 'o', 'i', 'l', 't', 'e', ' ', 'v', '.', 'u', 'h', 'r', 'd'}\n",
            "6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vY3Y_4kIbpaU"
      },
      "source": [
        "A nested list is a list within another list. You can iterate through nested lists in the following way:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AW8w46PbuFM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "641bade7-a2a3-4563-debb-1969cffbbd35"
      },
      "source": [
        "# A list of countries and their capitals within different continents\n",
        "continents = [\n",
        "    [('Iceland', 'Reykjav√≠k'), ('Germany', 'Berlin'), ('Spain', 'Madrid')],  # Europe\n",
        "    [('Japan', 'Tokyo'), ('China', 'Beijing'), ('South Korea', 'Seoul')],  # Asia\n",
        "    [('Nigeria', 'Abuja'), ('Algeria', 'Algiers'), ('Angola', 'Luanda')]  # Africa\n",
        "]\n",
        "\n",
        "# Create a list of all the countries in the previous list\n",
        "[country for continent in continents for (country, capital) in continent]"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Iceland',\n",
              " 'Germany',\n",
              " 'Spain',\n",
              " 'Japan',\n",
              " 'China',\n",
              " 'South Korea',\n",
              " 'Nigeria',\n",
              " 'Algeria',\n",
              " 'Angola']"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBrOT1vNcYZ0"
      },
      "source": [
        "## Sentiment analysis with NLTK\n",
        "[Chapter 6](https://www.nltk.org/book/ch06.html) of the NLTK book shows how the toolkit can be used to create document classifiers, including a sentiment analyzer. The NLTK includes the `movie_reviews` corpus, which contains 2,000 movie reviews. Half of the reviews have been labelled as **positive** and the other half as **negative**. Let's download it and take a look:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xx01nR8x9qgq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "434b5321-7076-4ca7-8254-cf9e6af87cb0"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import movie_reviews\n",
        "nltk.download('punkt')\n",
        "\n",
        "nltk.download('movie_reviews')\n",
        "print(\"Categories:\", movie_reviews.categories())"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Categories: ['neg', 'pos']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AppRxScjc9vx"
      },
      "source": [
        "As expected, there are two categories: `pos` for positive reviews and `neg` for negative reviews. For this particular corpus, each review is stored as a separate text file. To get a list of all the text files in the corpus, we can use `movie_reviews.fileids()`. We can also get a list of files for a specific category:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GI8EjJoo_1jz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7dee803f-d69e-4f98-c1b4-32b10ffc24b0"
      },
      "source": [
        "pos_fileids = movie_reviews.fileids('pos')\n",
        "neg_fileids = movie_reviews.fileids('neg')\n",
        "\n",
        "print(pos_fileids[:5])  # The first 5 positive reviews\n",
        "print(neg_fileids[:5])  # The first 5 negative reviews"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['pos/cv000_29590.txt', 'pos/cv001_18431.txt', 'pos/cv002_15918.txt', 'pos/cv003_11664.txt', 'pos/cv004_11636.txt']\n",
            "['neg/cv000_29416.txt', 'neg/cv001_19502.txt', 'neg/cv002_17424.txt', 'neg/cv003_12683.txt', 'neg/cv004_12641.txt']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQ_C9_80AEDo"
      },
      "source": [
        "We can get a list of all the tokens in the corpus with `movie_reviews.words()`. We can also specify a filename to get a single tokenized review:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZ8mPNPGcnbN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88c38e3e-89e1-4ce7-8150-36b9711e02f9"
      },
      "source": [
        "pos_reviews = [movie_reviews.words(fid) for fid in pos_fileids]\n",
        "neg_reviews = [movie_reviews.words(fid) for fid in neg_fileids]\n",
        "\n",
        "print(pos_reviews[0][:10])  # The first 10 tokens of the first positive review\n",
        "print(neg_reviews[0][:10])  # The first 10 tokens of the first negative review"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['films', 'adapted', 'from', 'comic', 'books', 'have', 'had', 'plenty', 'of', 'success']\n",
            "['plot', ':', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g04vosHorN6F"
      },
      "source": [
        "Some words, such as *brilliant* and *memorable*, are more strongly associated with positive reviews than negative ones. Similarly, *boring* and *unfunny* have a stronger association with negative reviews.\n",
        "\n",
        "Using the movie review corpus, we can train a classifier to predict whether a given review is positive or negative. The classifier extracts a set of *features* from every review, which are then used to make the classification. In this case, the features we use will be a dictionary that tells us whether each of the 2,000 most common words in the corpus is present within a review or not."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeixOJ-lu1fD"
      },
      "source": [
        "# Create a set with 2,000 of the most frequent words in the movie review corpus\n",
        "movie_fd = nltk.FreqDist(movie_reviews.words())\n",
        "movie_words = {word for word, count in movie_fd.most_common(2000)}\n",
        "\n",
        "# For a given review (in the form of a list or set of tokens), create a\n",
        "# dictionary which tells us which words are present and which are not.\n",
        "def get_review_features(review):\n",
        "  review_words = set(review)\n",
        "  return {word: word in review_words for word in movie_words}"
      ],
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-r6kXma8-eB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b388005-c787-45e0-ba2d-087c79c4496a"
      },
      "source": [
        "# Let's see how this works for the first positive review:\n",
        "example_features = get_review_features(pos_reviews[0])\n",
        "print(example_features)\n",
        "print(\"'funny' is in the review:\", example_features['funny'])\n",
        "print(\"'boring' is in the review:\", example_features['boring'])"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'this': True, 'club': False, 'boys': False, 'live': False, 'decent': False, 'start': False, 'men': False, 'called': True, 'shame': False, 'shooting': False, 'voice': False, 'project': False, 'children': False, 'solid': True, 'british': True, 'things': False, 'finds': False, 'tone': False, 'impressive': False, 'attempt': True, 'amusing': False, 'lacks': False, 'doubt': False, 'missing': False, 'man': False, 'done': False, 'silent': False, 'step': False, 'aspect': False, 'meaning': False, 'intriguing': False, 'thing': True, 'himself': False, 'third': False, 'd': False, 'secret': True, 'parts': False, 'test': False, 'utterly': False, 'south': False, 'scale': False, 'decided': False, 'jennifer': False, 'william': False, 'reach': False, 'tommy': False, 'he': True, 'goes': False, 'sitting': False, 'law': False, 'various': False, 'george': False, 'development': False, 'happens': False, 'teacher': False, 'arts': False, 'growing': False, '--': False, 'totally': False, 'hand': False, 'made': True, 'opinion': False, 'hands': False, 'hopes': False, 'wrote': False, 'animation': False, 'sick': False, 'popular': False, 'c': False, 'witch': False, 'producers': False, 'actions': False, 'supporting': True, 'sharp': False, 'married': False, 'familiar': False, 'thinking': False, 'hunt': False, 'hill': False, 'school': False, 'central': False, 'having': False, 'mr': False, 'shouldn': False, 'hasn': False, 'appreciate': False, 'necessary': False, 'gives': False, 'entirely': False, 'attractive': False, 'peter': True, 'surprise': True, 'plans': False, 'chan': False, 'felt': False, 'wasn': True, 'wait': False, 'angels': False, 'park': False, 'actor': False, 'haven': False, 'stands': False, 'blame': True, 'total': False, 'appearance': True, 'humorous': False, 'camp': False, 'rate': False, 'professional': False, 'gangster': False, 'effects': False, 'sequences': False, 'ok': False, 'badly': False, 'taylor': False, 'spectacular': False, 'then': False, 'bored': False, 'concept': False, 'stories': False, 'involved': False, 'major': False, 'gore': True, 'seagal': False, 'worst': False, '-': True, 'society': True, 'star': True, 'second': False, 'highly': False, 'contrived': False, 'white': True, 'jimmy': False, 'dennis': False, 'ride': False, 'touching': False, 'death': False, 'money': False, 'fly': False, 'rules': False, 'deliver': False, 'please': False, 'wasted': False, 'most': False, 'becomes': False, 'number': False, 'wrong': False, 'kill': False, 'pg': False, 'daughter': False, 'who': True, 'along': False, 'filled': False, 'middle': False, 'batman': True, 'joke': False, 'offer': False, 'modern': False, 'agent': False, 'era': True, 'cheesy': False, 'allows': False, 'surprisingly': False, 'remake': False, 'critics': False, 'guy': False, 'three': False, 'attempts': False, 'came': False, 'enjoy': False, 'urban': False, 'charming': False, 'version': False, '*': False, 'lots': False, 'serve': False, 'whom': False, 'background': False, 'nowhere': False, 'effective': False, 'hurt': False, 'scary': False, 'appeal': False, 'by': True, 'becoming': False, 'realized': False, 'air': False, 'ups': False, 'literally': False, 'process': False, 'inside': False, 'relationship': False, 'what': False, 'one': True, 'powers': False, 'moving': False, 'none': False, 'worthy': False, 'question': True, 'flat': False, 'beyond': False, '!': False, 'matt': False, 'half': True, 'night': False, 'willing': False, 'cameron': False, 'ex': False, 'hits': False, 'child': False, 'problems': False, 'women': False, 'approach': False, 'bringing': False, 'nature': False, 'introduced': False, 'left': False, 'known': False, 'loved': False, 'amazing': False, 'supposedly': False, 'teen': False, 'science': False, 'developed': False, 'jackson': True, 'too': False, 'offensive': False, 'anderson': False, 'indeed': False, 'they': True, 'steven': False, 'truman': False, 'takes': False, 'court': False, 'understand': False, 'wit': False, 'off': False, 'perhaps': False, 'typical': False, 'basically': False, 'but': True, 'plane': False, 'student': False, 'show': False, 'minutes': False, 'know': False, 'grace': False, 'hall': False, 'merely': False, 'surprising': True, 'etc': False, 'been': True, 'humor': False, 'sweet': False, '1': False, 'failed': False, 'character': False, 'stage': False, 'field': False, 'produced': False, 'ways': False, 'industry': False, 'system': False, 'comedy': False, 'believes': False, 'follows': False, 'latest': False, 'romantic': False, 'shows': False, 'gibson': False, 'mark': False, 'how': True, 'genuine': False, 'saw': True, 'henry': False, 'though': True, 'office': False, 'speech': False, 'g': False, 'eventually': False, 'austin': False, 'fake': False, 'subplot': False, 'files': False, 'magic': False, 'determined': False, 'wondering': False, 'said': False, 'girlfriend': False, 'for': True, 'anyway': False, 'final': False, 'particularly': False, '6': False, 'girls': False, '?': True, 'right': False, 'waiting': False, 'move': False, 'center': False, 'surprised': False, 'myself': False, 'again': False, 'sign': False, 'screen': False, 'personal': False, 'cop': False, 'should': False, 'creative': False, 'within': False, 'run': False, 'added': False, 'match': False, 'trailer': False, 'costumes': False, 'length': False, 'soon': False, 'fall': False, 'stone': False, 'fat': False, '90': False, 'giving': False, 'answer': False, 'rival': False, 'keep': False, 'mysterious': True, 'another': True, 'monster': False, 'singer': False, 'carter': False, 'forward': False, 'wonderful': False, 'zero': False, 'strike': False, 'sort': False, 'career': False, 'russell': False, 'just': False, 'help': False, 'eyes': False, 'rescue': False, 'matter': False, 'similar': False, 'fine': False, 'business': False, 'aspects': False, 'heads': False, 'both': True, 'tim': True, 'fair': False, 'visuals': False, 'chris': False, 'mind': False, 'adults': False, 'christopher': False, 'villain': False, 'lover': False, 'worse': False, 'officer': False, 'wouldn': False, 'further': False, 'bright': False, 'song': True, 'in': True, 'keeping': True, 'chosen': False, 'die': False, 'feelings': False, 'wall': False, 'clever': False, 'ahead': False, 'book': True, 'we': False, 'famous': False, 'big': True, 'realizes': False, 'fresh': False, 'quickly': False, 'mission': False, 'human': False, '`': False, 'summer': False, 'princess': False, 'sets': False, 'likable': False, 'class': False, 'learns': False, 'low': False, 'dealing': False, 'troopers': False, 'return': False, 'play': False, 'turning': True, 'lead': False, 'involving': False, 'bug': False, 'beast': False, 'succeeds': False, 'enjoyed': False, 'stop': False, 'save': False, 'almost': True, 'west': False, 'example': False, 'tarzan': False, 'powerful': False, 'narrative': False, 'side': False, 'bond': False, 'brother': False, '=': False, 'year': False, 'featuring': False, 'shown': False, 'while': False, 'lucky': False, 'york': False, 'porn': False, 'okay': False, 'begin': False, 'sequel': False, 'other': True, 'funniest': False, 'taking': False, 'instead': False, 'budget': False, 'following': False, 'directly': False, 'books': True, 'dvd': False, 'elizabeth': False, 'thanks': False, 'true': False, 'starship': False, 'days': True, 'apparent': False, 'position': False, 'maybe': False, 'king': False, 'sees': False, 'father': False, 'thrown': False, 'themes': False, 'washington': False, 'home': False, 'possibly': False, 'woody': False, 'weren': False, 'football': False, 'leader': False, 'killing': True, 'actress': False, '.': True, 'store': False, 'surface': False, 'accent': True, 'make': True, 'steve': True, 'thin': False, 'at': True, 'wild': False, 'cinematic': False, 'damn': False, 'revenge': False, 'odd': True, 'pleasure': False, 'found': False, 'perfect': False, 'herself': False, 'street': True, 'include': False, 'date': False, 'player': False, 'welcome': False, 'sheer': False, 'smart': False, 'meant': False, 'lack': False, 'virtually': False, 'engaging': False, 'frank': False, 'safe': False, 'whatever': False, 'spice': False, 'roberts': False, 'change': False, 'imagine': False, 'such': True, 's': True, 'are': True, 'cinematography': False, 'innocent': False, 'didn': False, 'from': True, 'complex': False, 'like': True, 'situations': False, 'shot': False, '$': False, 'top': True, 'escape': False, 'product': False, 'am': False, 'explained': False, 'performances': False, 'itself': False, 'amount': False, 'forgotten': False, 'difficult': False, 'hardly': False, 'premise': False, 'breaks': False, 'core': False, 'starring': False, 'wonder': False, 'teenage': False, 'tension': False, 'water': False, 'uses': False, 'excellent': False, 'decade': False, 'car': True, 'blue': False, 'situation': False, 'angry': False, 'guess': False, 'lawyer': False, 'motion': False, 'emotional': False, 'provides': False, 'hey': False, 'matrix': False, 'identity': True, 'charm': False, 'once': False, 'yourself': False, 'cold': False, 'someone': False, 'de': False, 'fail': False, 'red': False, 'video': False, 'needed': False, 'dr': False, 'episode': False, 'poor': False, 'o': False, 'strong': True, 'chase': False, 'than': True, 'face': False, 'formula': False, 'involves': False, 'obviously': False, 'guilty': False, 'caught': False, 'be': True, 'beach': False, 'm': False, 'beat': False, 'weak': False, 'needs': True, 'full': False, 'partner': False, 'overall': False, 'results': False, 'sister': False, 'theaters': False, 'allow': False, 'during': False, 'stars': False, 'toward': True, 'your': False, 'public': False, 'flynt': False, 'ideas': False, 'comedies': False, 'scream': False, 'neither': False, 'act': False, 'u': False, 'previous': False, 'goal': False, 'mel': False, '1998': False, 'lee': False, 'meet': False, 'filmmaking': False, 'believe': False, 'energy': False, 'security': False, 're': True, 'political': False, 'evil': False, 'have': True, 'war': False, 'happened': False, 'cage': False, 'there': True, 'mother': False, 'wife': False, 'follow': False, 'appear': False, 'constantly': False, 'outside': False, 'soul': False, 'mary': True, 'laughable': False, 'cash': False, 'no': True, 'admit': False, 'characters': False, 'creates': False, 'liked': False, 'genius': False, 'buddy': False, 'contains': False, 'impact': False, 'believable': False, 'cult': False, 'dance': False, 'brown': False, 'information': False, 'next': False, 'television': False, 'enjoyable': False, 'hotel': False, 'intelligence': False, 'add': False, 'jean': False, 'despite': False, 'blade': False, 'however': True, 'climax': False, 'technical': False, 'nicely': False, 'usually': False, 'all': True, 'america': False, 'sub': False, 'portrayed': False, 'government': False, 'giant': False, 'theater': False, 'kong': False, 'jones': False, 'audience': False, 'dimensional': False, 'rated': False, 'carrey': False, 'built': False, 'learn': False, 'bottom': False, 'gun': False, 'master': False, 'due': False, 'otherwise': False, 'bob': False, 'stuck': False, 'head': False, 'substance': False, 'mix': False, 'moral': False, 'dull': False, 'except': False, 'mess': False, 'japanese': False, 'view': False, 'house': False, 'nearly': True, 'gone': False, 'opposite': False, 'wonderfully': False, 'ray': False, 'best': False, 'them': False, 'way': False, 'does': False, 'above': False, 'followed': False, 'annoying': False, 'states': False, 'won': True, 'mediocre': False, 'light': False, 'spirit': False, 'pay': False, 'course': True, 'would': True, 'thinks': False, 'even': True, 'see': True, 'charles': False, 'finale': False, 'lord': False, 'set': True, 'their': True, 'near': False, 'mood': False, 'board': False, 'proves': False, 'bizarre': False, 'wanted': False, 'before': True, 'plain': False, 'bus': False, 'straight': False, 'tries': True, 'friends': False, 'bunch': False, 'opening': False, 'available': False, 'opens': False, 'devil': False, 'filmed': False, 'slapstick': False, 'season': False, 'towards': False, 'screenwriter': False, 'filmmaker': False, 'simply': False, 'emotion': False, 'comic': True, 'rating': False, 'always': False, 'larry': False, 'rich': False, 'join': False, 'older': False, 'relief': False, 'basic': False, 'usual': False, 'lies': False, 'accept': False, 'idea': False, 'screenplay': False, 'day': False, 'same': False, 'nor': False, 'seem': False, 'seat': False, 'l': False, 'crew': False, 'confusing': False, 'went': False, 'ridiculous': False, 'around': False, 'under': False, 'these': False, 'dramatic': False, 'movie': False, 'patch': False, 'seemed': False, 'rush': False, 'conflict': False, 'first': True, 'drug': True, 'realistic': False, 'chemistry': False, 'details': False, 'least': False, 'language': True, 'twice': False, 'twist': False, 'look': True, 'pathetic': False, 'not': True, 'post': False, 'picture': False, 'obvious': False, 'presents': False, 'compared': False, 'telling': False, 'masterpiece': False, 'it': True, 'series': True, 'brian': False, 'treat': False, 'protagonist': False, 'actors': False, 'interested': False, 'success': True, 'spent': False, 'reading': False, 'part': True, 'personality': False, 'sam': False, 'reeves': False, 'work': False, 'humans': False, 'limited': False, 'looks': True, 'free': False, 'boat': False, 'put': False, 'yet': False, 'down': False, 'company': False, 'several': False, 'compelling': False, 'score': False, 'thomas': False, 'fox': False, 'hong': False, 'ass': False, 'count': False, 'apes': True, 'biggest': False, 'attention': False, 'years': False, 'released': False, 'expected': False, 'walk': False, 'cheap': False, 'f': False, 'crap': False, 'matters': False, 'eight': False, 'through': True, 'short': False, 'laughing': False, 'laugh': False, 'presented': False, 'claire': False, 'gets': False, 'wedding': False, 'cartoon': False, 'started': False, 'ed': False, 'stephen': False, 'husband': False, 'very': True, 'taken': False, 'matthew': False, 'whose': False, 'lines': False, 'quality': False, 'experience': False, 'alien': False, 'coming': False, 'van': False, ';': False, 'event': False, 'cross': False, 'portrayal': False, 'prove': False, 'a': True, 'producer': False, 'ready': False, 'cast': False, 'did': True, 'century': False, 'respect': False, 'legend': False, 'll': True, 'whether': True, 'presence': False, 'device': False, 'effort': False, 'directors': True, 'feet': False, 'script': False, 'ended': False, 'isn': True, 'gay': False, 'amy': False, \"'\": True, 'far': False, 'natural': False, 'potential': False, 'incredible': False, 'queen': False, 'attack': False, 'became': False, 'fellow': False, 'paul': False, 'accident': False, 'miss': False, 'twenty': False, 'power': False, 'try': False, 'ship': False, 'normal': False, 'ryan': False, 'fiction': False, 'kevin': False, 'feel': False, 'male': False, 'private': False, 'talented': False, 'music': True, 'cut': False, 'brooks': False, 'rest': False, 'writers': False, 'violence': True, 'now': True, 'moments': False, 'throw': False, 'thus': False, 'up': True, 'casting': True, 'earth': False, 'fight': False, 'damme': False, 'nuclear': False, 'director': False, 'kills': False, '3': False, 'generated': False, 'write': False, 'grant': False, 'guard': False, 'fire': False, 'billy': False, 'streets': False, 'hope': False, 'plan': False, 'task': False, 'features': True, 'stunning': False, 'else': False, 'killer': True, 'meets': False, ')': True, 'station': False, 'already': False, 'strange': False, 'design': True, 'minor': False, ',': True, 'horror': False, 'plenty': True, 'names': False, 'unique': True, 'routine': False, 'message': False, 'silly': False, 'sequence': False, '\"': True, 'julia': False, 'slasher': False, 'reviews': False, 'dangerous': False, 'eccentric': False, 'gang': False, 'liners': False, 'musical': False, 'looking': False, 'acting': True, 'aren': False, 'million': False, 'likes': False, 'anthony': False, 'films': True, 'bruce': False, 'president': False, '100': False, 'unlike': False, 'desperate': False, 'professor': False, 'send': False, 'planet': True, 'puts': False, 'menace': True, 'wish': False, 'grand': False, 'subject': True, '1997': False, 'thought': False, 'charlie': False, 'ii': True, 'particular': False, 'although': False, 'going': False, 'cover': False, 'group': False, 'expectations': False, 'check': False, 'nice': False, 'reality': False, 'dies': False, 'sexual': False, 'others': False, 'order': False, 'inevitable': False, 'horse': False, 'reveal': False, 'pain': False, 'johnny': True, 'brings': False, 'thriller': False, 'catch': False, 'species': False, 'members': False, 'helps': False, 'force': False, 'manages': False, 'looked': False, 'every': False, 'twists': False, 'disappointing': False, 'brought': True, 'doing': False, 'yeah': False, 'line': False, 'hanks': False, 'criminal': False, 'lets': False, 'conclusion': False, 'sight': False, 'direct': True, 'julie': False, 'pure': False, 'body': False, 'certainly': True, 'together': False, 'late': False, 'state': False, 'fully': False, 'shoot': False, 'quite': False, 'over': True, 'soundtrack': False, 'buy': False, 'fare': False, 'hoping': False, 'blood': False, 'without': False, 'heavy': False, 'impossible': False, 'past': True, 'party': False, 'dialogue': False, 'knows': False, 'media': False, 'probably': False, 'titanic': False, 'world': True, 'original': True, 'break': False, 'complete': False, 'case': True, 'many': False, 'here': True, 'max': False, 'physical': False, 'might': True, 'joe': True, 'must': False, 'promise': False, 'jane': False, 'dad': False, 'press': False, 'ted': False, 'robert': False, 'apparently': False, 'terms': False, 'actually': True, 'action': False, 'former': False, 'where': True, '20': False, 'self': False, 'sorry': False, 'writer': False, 'drugs': False, 'shots': False, 'jim': False, 'ultimately': False, 'seeing': False, 'loves': False, 'suppose': False, 'williams': False, 'fate': False, 'al': False, 'decision': False, 'away': False, 'do': True, 'fascinating': False, 'expect': False, 'trip': False, 'individual': False, 'front': False, 'lose': False, 'tale': False, 'emotions': False, 'art': False, 'winner': True, 'suspects': False, 'news': False, 'ends': False, 'control': False, 'imagination': False, 'sound': False, 'actual': False, 'find': True, 'hours': False, 'nick': False, 'inspired': False, 'allen': True, 'jerry': False, 'story': False, 'local': False, 'poorly': False, 'open': False, 'ill': False, 'or': True, 'boss': False, 'parody': False, 'people': False, 'remember': False, 'jack': True, 'heard': False, 'rent': False, 'go': True, 'completely': True, 'great': True, 'creepy': True, 'when': True, 'road': False, 'eddie': True, 'loving': False, 'much': True, 'better': True, 'somewhere': False, 'never': True, 'our': False, 'girl': False, 'dark': True, 'starts': False, 'meeting': False, 'adds': False, 'meanwhile': False, 'french': False, 'take': False, 'heroes': False, 'credit': False, 'honest': False, 'exception': False, 'vampires': False, 'gold': False, 'explain': False, '4': False, 'saving': False, 'box': False, 'seen': False, 'excuse': False, 'exist': False, 'game': False, 'jr': False, 'graphic': True, 'watching': False, 'those': False, 'fails': False, 'material': False, 'x': False, 'mrs': False, 'crowd': True, 'so': True, 'tv': False, 'pace': False, 'impression': False, 'adaptation': False, 'david': False, '&': False, 'mention': False, 'ford': False, 'history': False, 'feature': False, 'humanity': False, 'desire': False, 'created': True, 'pictures': False, 'lynch': False, 'captain': False, 'gary': False, 'naked': False, 'leading': False, 'deep': False, 'feeling': False, 'clearly': False, 'unfortunately': False, 'atmosphere': False, 'boyfriend': False, 'born': False, 'anything': True, 'army': False, 'type': False, 'memorable': False, 'result': False, 'huge': False, 'american': False, 'cute': False, 'really': True, 'suddenly': False, 'provide': False, 'animated': False, '9': False, 'players': False, 'blair': False, 'nudity': False, 'until': True, 'suspense': False, 'emotionally': False, 'throughout': False, 'title': False, 'terrific': False, 'relationships': False, 'frame': False, 'worked': False, 'person': False, 'to': True, 'sit': False, 'small': False, 'catherine': False, 'name': False, 'possible': False, 'leaving': False, 'may': False, 'exactly': False, 'smile': False, 'truly': False, 'gags': False, 'places': False, 'dog': False, 'different': False, 'job': True, 'black': True, 'single': False, 'addition': False, 'superb': False, 'vegas': False, 'winning': False, 'shock': False, 'apartment': False, 'playing': False, 'brilliant': False, 'word': True, 'mike': False, 'cruise': False, 'window': False, 'extreme': False, 'model': False, 'elements': False, 'talent': False, 'technology': False, 'jason': False, 'offers': False, 'its': True, 'were': False, 'bill': False, 'us': False, 'plays': False, 'successful': False, 'survive': False, 'r': True, 'clear': False, 'hospital': False, 'favorite': False, 'wants': False, 'rising': False, 'apart': False, 'end': True, 'computer': False, 'appealing': False, 'forever': False, 'become': False, 'door': False, 'between': False, 'give': False, 'parents': False, 'simon': False, 'intended': False, 'directed': False, 'talk': False, 'role': False, 'tried': False, 'feels': False, 'alive': False, 'running': False, 'create': False, 'edge': False, 'joan': False, 'b': False, 'fans': False, 'easily': False, 'fairly': False, 'team': False, 'rock': False, 'suspect': False, 'get': True, 'characterization': False, 'focus': False, 'occasionally': False, 'family': False, 'cameo': False, 'bring': False, 'changed': False, 'come': False, 'saying': True, 'beautiful': False, 'build': False, 'recently': False, 'kind': False, 'earlier': False, 'hour': False, 'entertaining': False, 'chance': False, 'minute': False, 'enough': True, 'battle': False, 'ago': False, 'stuart': False, 'events': False, 'j': False, 'pull': False, 'ask': False, 've': False, 'generally': False, 'absolutely': False, 'godzilla': False, 'an': True, 'theme': False, 'cinema': False, 'period': False, 'average': False, 'steal': False, 'love': True, 'film': True, 'truth': False, 'teenagers': False, 'instance': False, 'studio': False, 'jokes': False, 'upon': True, 'dumb': False, 'across': False, 'slowly': False, 'witty': False, 'deal': False, 'his': False, 'comedic': False, 'asked': False, 'friend': False, 'stupid': False, 'destroy': False, 'victim': False, 'remains': False, 'surprises': False, 'her': True, 'town': False, 'guns': False, 'will': True, 'lives': False, 'boring': False, 'with': True, 'responsible': False, 'ten': False, 'mad': True, 'hopkins': False, 'mostly': False, 'call': False, 'stay': False, 'robin': False, 'gave': False, 'walking': False, 'trouble': False, 'she': True, 'superior': False, 'seemingly': False, 'today': False, 'cause': False, 'marriage': False, 'began': False, 'jackie': False, 'trek': False, 'songs': False, 'sometimes': False, 'list': False, 'pointless': False, 'speaking': False, 'six': False, 'track': False, 'since': False, 'speak': False, 'son': False, 'among': False, 'life': False, 'spawn': True, 'any': False, 'wars': False, 'into': True, 'nasty': False, 'taste': False, 'helen': False, 'share': False, 'that': True, 'trying': False, 'consider': False, 'read': False, 'as': True, 'tarantino': False, 'was': True, 'footage': False, 'clich': False, 'murphy': False, 'cannot': False, 'yes': False, 'ms': False, 'campbell': True, 'me': True, 'slightly': False, 'romance': False, 'making': False, 'lady': False, 'killed': False, 'skills': False, 'general': False, 'seconds': False, 'turn': False, 'damon': False, 'decides': False, 'questions': False, 'martial': False, 'after': True, 'vampire': False, 'ground': False, 'content': True, 'stuff': False, 'sean': False, 'tell': False, 'calls': True, 'old': False, 'used': False, 'forced': False, 'makes': False, 'stand': False, 'journey': False, 'setting': False, 'sadly': False, 'fi': False, 'kept': False, 'flying': False, 'writing': False, 'space': False, 'fantastic': False, 'could': True, 'epic': False, 'recent': False, 'strength': False, 'entertainment': False, 'told': False, 'still': False, 'longer': False, 'also': False, 'e': False, 'snake': False, 'knew': False, 'last': False, 'sounds': False, 'annie': False, 'some': False, 'arnold': False, 'therefore': False, 'asks': False, 'scene': False, 'own': False, 'point': True, 'building': False, 'unless': False, '10': False, 'breaking': False, 'faith': False, 'everyone': False, 'haunting': False, '5': False, 'nights': False, 'sense': True, 'laughs': False, 'dollars': False, 'funny': True, 'appropriate': False, 'lot': False, 'jeff': False, 'aside': False, 'forget': False, 'dreams': True, 'adventure': False, 'hilarious': False, 'interest': False, 'expecting': False, 'standing': False, 'crime': True, 'took': False, 'disturbing': False, 'social': False, 'wide': False, 'joel': False, 'anti': False, 'miller': False, 'leads': False, 'edward': False, 'him': True, 'happy': False, 'bar': False, 'manner': False, 'kid': False, 'given': False, 'wise': False, 'keeps': False, 'fault': False, 'machine': False, 'cares': False, 'ever': False, 'back': True, 'considered': False, 'roles': True, 'more': True, 'manage': False, 'fantasy': False, 'died': False, 'element': False, 'awful': False, 'folks': False, 'patrick': False, 'ice': False, 'leave': False, 'disney': False, 'city': False, 'falls': False, 'quick': False, 'ones': False, 'toy': False, 'constant': False, 'special': False, 'reason': False, 'based': False, 'has': True, 'realize': False, 'ability': False, 'my': False, 'fighting': False, ':': True, 'acts': True, 'seriously': False, 'band': False, 'bland': False, 'affleck': False, 'nobody': False, '1999': False, 'scientist': False, 'being': False, 'young': False, 'plus': False, 'sympathetic': False, 'phone': False, 'thoroughly': True, 'christmas': False, 'baby': False, 'watch': True, 'fact': False, 'barely': False, 'contact': False, 'mulan': False, 'violent': True, 'satire': False, 'only': False, 'carpenter': False, 'plot': False, 'fame': False, 'forces': False, 'problem': False, 'standard': False, 'filmmakers': False, 'doesn': False, 'note': False, 'waste': False, 'broken': False, 'leaves': False, 'age': False, 'smith': False, 'phantom': False, 'moore': True, 'against': False, 'key': False, 'woo': False, '[': False, 'difference': False, 'begins': False, 'headed': False, 'supposed': False, ']': False, 'hair': False, '(': True, 'effect': False, 'discovers': False, 'appears': False, 'had': True, 'nothing': True, 'harry': False, 'pretty': False, 'long': True, 'carry': False, 'jail': False, 'disaster': False, 'faces': False, 'holds': True, 'fear': False, 'production': True, 'jedi': False, 'god': False, 'sent': False, 'travel': False, 'originally': False, 'win': False, 'about': True, 'military': False, 'hidden': True, 'time': True, 'adult': False, 'spielberg': False, 'burton': True, 'provided': False, 'willis': False, 'bloody': False, 'eye': False, 'genre': False, 'happen': False, 'brothers': True, 'and': True, 'radio': False, 'creating': False, 'subtle': False, 'think': True, 'good': True, 'moves': False, 'member': False, 'rocky': False, 'touch': False, 'got': False, 'opportunity': False, '1996': False, 'points': False, 'if': True, 'living': False, 'guys': False, 'rob': False, 'generation': False, 'comparison': True, 'detail': False, 'virus': False, 'easy': False, 'led': False, 'flaws': False, '8': False, 'less': False, 'attitude': False, 'kate': False, 'thankfully': False, '+': False, 'able': False, 'ghost': True, 'trust': False, 'review': False, 'dead': False, 'room': False, 'loud': False, 'heart': False, 'brain': False, 'fast': False, 'country': False, 'tough': False, 'hollywood': False, 'serious': False, 'viewing': False, 'five': False, 'driver': False, 'singing': False, 'beauty': False, 'lost': False, 'sure': False, 'later': False, 'police': True, 'deserves': False, 'everything': False, 'failure': False, 'price': False, 'high': False, 'anyone': True, 'cops': False, 'james': False, 'recommend': False, 'support': False, 'hear': False, 'ben': False, 'hot': False, 'fashion': False, 'paced': False, 'early': False, 'runs': False, 'placed': False, 'culture': False, 'travolta': False, 'tired': False, 'say': True, 'mob': False, 'john': False, 'green': False, 'present': False, 'predictable': False, 'cinematographer': True, 'remarkable': False, 'need': False, 'humour': False, 'real': False, 'written': False, 'search': False, 'terrible': False, 'oh': False, 'baldwin': False, 'managed': False, 'prison': False, 'which': False, 'works': False, 'kiss': False, 'sci': False, 'united': False, '7': False, 'latter': False, 'issues': False, 'turned': False, 'color': True, 'comes': False, 'likely': False, 'can': True, 'considering': False, 'tells': False, 'sad': False, 'audiences': False, 'behind': True, 'hero': False, 'working': False, 'intelligent': False, 'out': False, 'female': False, 'detective': False, 'using': False, 'fbi': False, 'whole': True, 'equally': False, 'fit': False, 'definitely': False, 'scenes': True, 'i': True, 'directing': False, 'discover': False, 'confused': False, 'current': False, 'bad': True, 'hard': False, 'played': False, 'grows': False, 'murder': False, 'danny': False, 'rare': False, 'something': False, 'drive': False, 'vincent': False, 'acted': False, 'credits': False, 'community': False, 'close': False, 'large': False, 'changes': False, 'onto': True, 'flicks': False, 'target': False, 'dream': False, 'woods': False, 'creature': False, 'anne': False, 'doctor': False, 'use': False, 'kelly': True, 'months': False, 'bit': False, 'depth': False, 'care': False, 'cliches': False, 'stock': False, 'you': True, 'michael': True, 'fun': False, 'reasons': True, 'delivers': False, 'talking': False, 'well': True, 'ugly': False, 'drawn': False, 'sandler': False, 'alex': False, 'direction': False, 'myers': False, 'important': False, 'quiet': False, 'is': True, 'mars': False, '/': True, 'lucas': False, 'mystery': False, 'hell': True, 'mouth': True, 'editing': False, 'lame': False, 'food': False, 'viewers': True, 'images': False, 'younger': False, 'especially': False, 'train': False, 'douglas': False, 'interesting': True, 'falling': False, 'exciting': False, 'rarely': False, 'finally': False, 'greatest': False, '000': False, 'college': False, 'chinese': False, 'numerous': False, 'worth': False, 'each': False, 'four': False, 'figures': False, 'want': False, 'slow': False, 'hit': False, 'release': False, 'pulp': False, 'finding': False, 'driving': False, 'heaven': False, 'roger': False, 'simple': False, 'includes': True, 'common': False, 'purpose': False, 'certain': False, 'dying': False, 'place': True, 'novel': True, 'hate': False, 'let': False, 'named': True, 'storyline': False, 'shallow': False, 'deals': False, 'owner': False, 'beginning': False, 'land': False, 'hold': False, 'english': False, '2': True, 'ending': True, 'jay': False, 'ultimate': False, 'critic': False, 'tony': False, 'pieces': False, 'debut': False, 'often': False, 'either': False, 'new': True, 'la': False, 'artist': False, 'fan': False, 'two': False, 'double': False, 'classic': False, 'level': True, 'convincing': False, 'soldiers': False, 'dude': False, 'somewhat': False, 'spend': False, 'aliens': False, 'times': True, 'few': False, 'oscar': True, 'camera': False, 'week': False, 'pop': False, 'villains': False, 'non': False, 'form': False, 'hunting': False, 'couple': False, 'scott': False, 'martin': True, 'driven': False, 'pair': False, 'future': False, 'loses': False, 'returns': False, 'why': False, 'watched': False, 'on': True, 'piece': False, 'richard': False, 'overly': False, 'mean': False, 'boy': False, 'williamson': False, 'image': False, 'because': True, 'spends': False, 'happening': False, 'means': False, 'extremely': False, 'continues': False, 'performance': True, 'documentary': False, 'pass': False, 'outstanding': False, 'getting': True, 'decide': False, 'speed': False, 'says': False, 'notice': False, 'couldn': False, 'brief': False, 'showing': False, 'incredibly': False, 'perfectly': False, '13': False, 'crash': False, 'choice': False, 'seven': False, 'visually': False, 'of': True, 'talents': False, 'sarah': False, 'alone': False, 'tom': False, 'award': False, 'sexy': False, 'super': False, 'turns': True, 'mentioned': False, 'weird': False, 'entire': False, 't': True, 'somehow': False, 'island': False, 'kids': True, 'rather': False, 'race': False, 'floor': False, 'unfunny': False, 'movies': False, 'woman': False, 'rose': False, 'shakespeare': True, 'flick': False, 'sex': False, 'crazy': True, 'frightening': False, 'alan': True, 'co': False, '30': True, 'pick': False, 'struggle': False, 'intense': False, 'little': True, 'immediately': False, 'horrible': False, 'figure': False, 'themselves': False, 'visit': False, 'seems': True, 'drama': False, 'the': True, 'babe': False, 'goofy': False, 'main': False, 'words': True, 'don': True, 'wayne': False, 'adams': False, 'including': False, 'viewer': False, 'style': False, 'visual': False, 'academy': False, 'avoid': False, 'affair': False, 'moment': False, 'cool': False}\n",
            "'funny' is in the review: True\n",
            "'boring' is in the review: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KK5rLkC7TCVC"
      },
      "source": [
        "Next, let's create a training set that we can use to train a Naive Bayesian classifier. The training set, in this case, is a list of tuples in the format `[(features, category), ...]`, where `features` is a dictionary from `get_review_features()` and `category` is either `pos` or `neg`, depending on whether the review is positive or negative. To get an idea of how well the classifier performs, we're going to reserve 10% of the reviews for testing. That means that we'll be training our classifier on 1800 examples and testing it on 200 examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqhiywiwrhxW"
      },
      "source": [
        "pos_examples = [(get_review_features(review), 'pos') for review in pos_reviews]\n",
        "neg_examples = [(get_review_features(review), 'neg') for review in neg_reviews]\n",
        "\n",
        "movie_training = pos_examples[:900] + neg_examples[:900]  # 1800 examples total\n",
        "movie_test = pos_examples[900:] + neg_examples[900:]  # 200 examples total"
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jfty2sigVuf3"
      },
      "source": [
        "Now we have everything we need to train our classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAqq1Z72wLHl"
      },
      "source": [
        "movie_classifier = nltk.NaiveBayesClassifier.train(movie_training)"
      ],
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3inpPaRDV0_6"
      },
      "source": [
        "How well does it perform on the test set?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7eJMkqdw7Eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1e945b4-db3d-4950-9a43-0e598cbf3c85"
      },
      "source": [
        "print(\"Accuracy:\", nltk.classify.accuracy(movie_classifier, movie_test))"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.815\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxakIFH5WNH_"
      },
      "source": [
        "The classifier achieves an accuracy of 81.5%. Let's take a look at which words have the biggest weights:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibMPC0QCxefo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da808b43-45cf-47de-c908-0b0db455847b"
      },
      "source": [
        "movie_classifier.show_most_informative_features(20)"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most Informative Features\n",
            "             outstanding = True              pos : neg    =     15.6 : 1.0\n",
            "                   mulan = True              pos : neg    =      9.0 : 1.0\n",
            "             wonderfully = True              pos : neg    =      7.1 : 1.0\n",
            "                  seagal = True              neg : pos    =      7.0 : 1.0\n",
            "                   damon = True              pos : neg    =      6.1 : 1.0\n",
            "                   flynt = True              pos : neg    =      5.7 : 1.0\n",
            "                  wasted = True              neg : pos    =      5.6 : 1.0\n",
            "                    lame = True              neg : pos    =      5.3 : 1.0\n",
            "                  poorly = True              neg : pos    =      5.2 : 1.0\n",
            "                   awful = True              neg : pos    =      4.9 : 1.0\n",
            "              ridiculous = True              neg : pos    =      4.8 : 1.0\n",
            "                    jedi = True              pos : neg    =      4.4 : 1.0\n",
            "                 unfunny = True              neg : pos    =      4.4 : 1.0\n",
            "                   waste = True              neg : pos    =      4.4 : 1.0\n",
            "               fantastic = True              pos : neg    =      4.4 : 1.0\n",
            "                   worst = True              neg : pos    =      4.2 : 1.0\n",
            "                    mess = True              neg : pos    =      4.2 : 1.0\n",
            "                  stupid = True              neg : pos    =      4.2 : 1.0\n",
            "                   bland = True              neg : pos    =      4.0 : 1.0\n",
            "                     era = True              pos : neg    =      4.0 : 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPSLZUxt9uMY"
      },
      "source": [
        "# Assignment\n",
        "Answer the following questions and hand in your solution in Canvas before 23:59 on September 6th. Remember to save your file before uploading it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQN5Qq9z97Yc"
      },
      "source": [
        "## Question 1\n",
        "The NLTK also includes a `subjectivity` corpus, which contains a collection of sentences that have either been categorized as **subjective** (emotional, expressing personal feelings and views)  or **objective** (more rational, factual). Some examples:\n",
        "\n",
        "* **Objective sentences**:\n",
        "  * uma thurman stars in quentin tarantino's fourth film venture , kill bill .  \n",
        "  * he lives in a motor garage with his six friends .\n",
        "  * the ensuing battle was one of the most savage in u . s . history .\n",
        "* **Subjective sentences**:\n",
        "  * seagal's strenuous attempt at a change in expression could very well clinch him this year's razzie .\n",
        "  * de niro cries . you'll cry for your money back .\n",
        "  * a heroic tale of persistence that is sure to win viewers' hearts .\n",
        "\n",
        "Unlike the movie review corpus, where every review is stored in separate file, here there is only one file for each category.\n",
        "\n",
        "Complete the following tasks:\n",
        "1. Import and download the `subjectivity` corpus.\n",
        "2. Find the names of each category.\n",
        "3. Using the category names, get the relative path of each file.\n",
        "4. Get a list of tokenized sentences for each category (using `subjectivity.sents(fileid)`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zc1EM7TvnYaC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15f01d64-7e1a-4f7f-9d12-47b5459baa35"
      },
      "source": [
        "# Your solution here\n",
        "# if not already imported import nltk\n",
        "import nltk\n",
        "from nltk.corpus import subjectivity\n",
        "\n",
        "# downloading the corpus \"subjectivity\" and punkt if isn't already present\n",
        "nltk.download('punkt')\n",
        "nltk.download('subjectivity')\n",
        "\n",
        "# find name of each category\n",
        "# print(f\"Categories: {subjectivity.categories()}\")\n",
        "\n",
        "# retrieve the relative path of each path\n",
        "obj_path = subjectivity.fileids('obj')\n",
        "subj_path = subjectivity.fileids('subj')\n",
        "\n",
        "# print(obj_path)\n",
        "# print(subj_path)\n",
        "\n",
        "# get a list of tokenized sentences\n",
        "list_sentences_obj = subjectivity.sents(obj_path)\n",
        "list_sentences_subj = subjectivity.sents(subj_path)\n",
        "\n",
        "print(list_sentences_obj)\n",
        "print(list_sentences_subj)"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['the', 'movie', 'begins', 'in', 'the', 'past', 'where', 'a', 'young', 'boy', 'named', 'sam', 'attempts', 'to', 'save', 'celebi', 'from', 'a', 'hunter', '.'], ['emerging', 'from', 'the', 'human', 'psyche', 'and', 'showing', 'characteristics', 'of', 'abstract', 'expressionism', ',', 'minimalism', 'and', 'russian', 'constructivism', ',', 'graffiti', 'removal', 'has', 'secured', 'its', 'place', 'in', 'the', 'history', 'of', 'modern', 'art', 'while', 'being', 'created', 'by', 'artists', 'who', 'are', 'unconscious', 'of', 'their', 'artistic', 'achievements', '.'], ...]\n",
            "[['smart', 'and', 'alert', ',', 'thirteen', 'conversations', 'about', 'one', 'thing', 'is', 'a', 'small', 'gem', '.'], ['color', ',', 'musical', 'bounce', 'and', 'warm', 'seas', 'lapping', 'on', 'island', 'shores', '.', 'and', 'just', 'enough', 'science', 'to', 'send', 'you', 'home', 'thinking', '.'], ...]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package subjectivity to /root/nltk_data...\n",
            "[nltk_data]   Package subjectivity is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaXJ-5Lr97_w"
      },
      "source": [
        "## Question 2\n",
        "Complete the following tasks:\n",
        "1. Create a set with the 2,000 most common words in the `subjectivity` corpus using `nltk.FreqDist()`.\n",
        "2. Create a function that takes a single, tokenized sentence as input (e.g., `['the', 'ensuing', 'battle', ...]`), and returns a dictionary of the 2,000 most frequent words and whether or not they are in the sentence (e.g., `{'battle': True, 'amusing': False, ...}`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Bx4kZSW98xq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c75a35e3-7eeb-4d4c-9b68-4fc078fb09fb"
      },
      "source": [
        "# Your solution here\n",
        "# get the distribution using FreqDist\n",
        "subject_fd = nltk.FreqDist(subjectivity.words())\n",
        "# using set comprehensions I get the most commond (2000) words and take in account\n",
        "# only `word` from the set since the set consist in a pair {word, count}.\n",
        "subject_word = {word for word, count in subject_fd.most_common(2000)}\n",
        "\n",
        "# function that takes a tokenized sentence and return if the token appears in\n",
        "# the most frequent words\n",
        "def get_token_sentence(sentence):\n",
        "  # extract the token and save in a set to avoid duplicate tokens\n",
        "  set_token = set(sentence)\n",
        "  return {token: token in subject_word for token in set_token}\n",
        "  # return {token:(True if token in subject_word else False) for token in set_token}\n",
        "  # return {token:  for token in set_token if token in subject_word}\n",
        "\n",
        "# See the behaviour using a sentence as example\n",
        "example = get_token_sentence(list_sentences_obj[54])\n",
        "print(example)\n"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'lead': True, 'that': True, 'molly': False, 'in': True, 'of': True, 'them': True, 'will': True, 'one': True, ',': True, 'ahead': True, '1': False, 'with': True, 'rabbit-proof': False, 'the': True, 'an': True, 'fence': False, 'authorities': False, 'continent': False, 'search': True, 'journey': True, 'determination': False, \"australia's\": False, 'outback': False, 'epic': True, 'home': True, 'step': True, '500': False, 'bisects': False, 'miles': False, 'over': True, 'grit': False, '.': True, 'on': True, 'guides': False, 'and': True, 'girls': True}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vrxhu0iB99YN"
      },
      "source": [
        "## Question 3\n",
        "Complete the following tasks:\n",
        "1. Create a training set with 9,000 sentences (4,500 of each category)\n",
        "2. Create a test set with 1,000 sentences (500 of each category)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08xWq_S899xK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af1ec1ab-992c-4f2a-d066-e4eee79fb66e"
      },
      "source": [
        "# Your solution here\n",
        "# Create sentences for each category\n",
        "obj_example = [(get_token_sentence(category), 'obj') for category in list_sentences_obj]\n",
        "subj_example = [(get_token_sentence(category), 'subj') for category in list_sentences_subj]\n",
        "\n",
        "# split set into training and test set\n",
        "subjective_training = obj_example[:4500] + subj_example[:4500]\n",
        "subjective_test = obj_example[4500:5000] + subj_example[4500:5000]\n",
        "\n",
        "# check if the training has 9k sentences nd if test has 1k\n",
        "print(f\"Length training: {len(subjective_training)}\")\n",
        "print(f\"Length test: {len(subjective_test)}\")\n"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length training: 9000\n",
            "Length test: 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HToPuGzX9-Cl"
      },
      "source": [
        "## Question 4\n",
        "Complete the following tasks:\n",
        "1. Train a Naive Bayes classifier using the training set from the previous question.\n",
        "2. Evaluate the classifier on the test set. How accurate is it?\n",
        "3. Find the 20 most informative features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHrN1tvKZa4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c4a872b-5f0f-461a-9eea-e6d294f244cf"
      },
      "source": [
        "# Your solution here\n",
        "\n",
        "# Train the Naive Bayes classifier\n",
        "subjective_clf = nltk.NaiveBayesClassifier.train(subjective_training)\n",
        "# Evaluation using test set to see the accuracy\n",
        "print(f\"Accuracy: {nltk.classify.accuracy(subjective_clf, subjective_test)}\")\n",
        "# See the most informative features\n",
        "subjective_clf.show_most_informative_features(20)"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.923\n",
            "Most Informative Features\n",
            "                      -- = True             subj : obj    =     70.1 : 1.0\n",
            "                   order = True              obj : subj   =     39.0 : 1.0\n",
            "                 decides = True              obj : subj   =     35.7 : 1.0\n",
            "                  sister = True              obj : subj   =     27.7 : 1.0\n",
            "            entertaining = True             subj : obj    =     26.6 : 1.0\n",
            "              girlfriend = True              obj : subj   =     26.3 : 1.0\n",
            "                discover = True              obj : subj   =     25.0 : 1.0\n",
            "                  film's = True             subj : obj    =     25.0 : 1.0\n",
            "                  you're = True             subj : obj    =     22.6 : 1.0\n",
            "                daughter = True              obj : subj   =     22.4 : 1.0\n",
            "                 married = True              obj : subj   =     21.7 : 1.0\n",
            "                 amusing = True             subj : obj    =     19.7 : 1.0\n",
            "                   plans = True              obj : subj   =     19.7 : 1.0\n",
            "                probably = True             subj : obj    =     19.7 : 1.0\n",
            "                    plan = True              obj : subj   =     19.4 : 1.0\n",
            "                    town = True              obj : subj   =     19.3 : 1.0\n",
            "                  you've = True             subj : obj    =     19.0 : 1.0\n",
            "                    kill = True              obj : subj   =     18.6 : 1.0\n",
            "                    slow = True             subj : obj    =     18.3 : 1.0\n",
            "             interesting = True             subj : obj    =     18.1 : 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 5\n",
        "Dialog acts are sort of the type of *action* performed by the speaker. In the instant messaging corpus dataset 'NPS', each utterance is labeled with one of 15 dialogue act types, such as **Statement**, **Emotion**, **ynQuestion**, **Continuer**, etc.\n",
        "\n",
        "Your task is to classify text from the NPS corpus into two dialog acts: **whQuestion** or **Emotion**.\n",
        "\n",
        "Start by downloading the NPS corpus and getting all posts from the corpus:"
      ],
      "metadata": {
        "id": "f-J9BEEQFrbo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('nps_chat')\n",
        "posts = nltk.corpus.nps_chat.xml_posts()"
      ],
      "metadata": {
        "id": "Qtc_pG1qJZgh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3445e31-ec24-4f44-dae6-6a158da70209"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]   Package nps_chat is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a list that only includes posts of class **Emotion** and **whQuestion**. You can access the class of a post by calling `post.get(\"class\")`."
      ],
      "metadata": {
        "id": "p0wQn19UqfKV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your solution here\n",
        "# print(posts)\n",
        "# list comprehension to get only post `Emotion` and `whQuestion`\n",
        "filtered = [(post.text, post.get(\"class\")) for post in posts if post.get(\"class\") == \"Emotion\" or post.get(\"class\") == \"whQuestion\"]\n",
        "# print(filtered)\n"
      ],
      "metadata": {
        "id": "H316uXLTrRUA"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Randomize the posts and create a training set and a test set, where the first 1300 **Emotion + whQuestion** posts are used for training and the rest for testing."
      ],
      "metadata": {
        "id": "t1K-jkXCzv4T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your solution here\n",
        "# Divide training and test set\n",
        "import random\n",
        "random.shuffle(filtered)\n",
        "training_set = filtered[:1300]\n",
        "test_set = filtered[1300:]\n",
        "# print(training_set)"
      ],
      "metadata": {
        "id": "1SQdx2rY0LG6"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a list of the 200 most frequent tokens in the training set. You can access the text of a `post` object by calling `post.text`. Remember that the **split** function will use whitespace to tokenize a string: `some_string.split()`"
      ],
      "metadata": {
        "id": "38Klc5bHxDvW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your solution here\n",
        "# Create a list of tokens from the training set\n",
        "tokens = [token for post, _ in training_set for token in post.split()]\n",
        "\n",
        "# Get the 200 most frequent tokens\n",
        "fd = nltk.FreqDist(tokens)\n",
        "most_frequent_tokens = [token for token, _ in fd.most_common(200)]\n",
        "\n",
        "# print(most_frequent_tokens)\n"
      ],
      "metadata": {
        "id": "cmBZL8wxso-q"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Define two feature selection functions that take a string as input and output a dictionary of features:\n",
        "* `get_word_features(string)`\n",
        "* `get_custom_features(string)`\n",
        "\n",
        "Begin by defining `get_word_features`. This function should use the words as features, just like in the movie review example above.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tyC-0es9KwTq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your solution here\n",
        "def get_word_features(post):\n",
        "  # words_review = set(post.split())\n",
        "  # words_review = post\n",
        "  return {token: token in post.lower() for token in most_frequent_tokens}\n",
        "  # return {token: True for token in most_frequent_tokens}"
      ],
      "metadata": {
        "id": "6b6mKRLiFqEo"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, define `get_custom_features`. This function should extract the features from the text that characterize the **Emotion** and **whQuestions** classes."
      ],
      "metadata": {
        "id": "n4LmO-UaJCHj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your solution here\n",
        "def get_custom_features(custom):\n",
        "  custom = custom.lower()\n",
        "  #ustom_tokens = set(custom.split())\n",
        "  # custom_tokens = custom.lower().split()\n",
        "  features = [\"who\", \"what\", \"when\", \"where\", \"why\", \"how\", \"!\", \"?\", \"lol\", \"hahaha\", \"haha\", \":)\", \":(\", \"lmao\", \"rofl\", \"teehee\", \"XD\", \":-)\", \"8D\", \":X\", \":-/\", \":/\", \"<3\", \":'(\"]\n",
        "  features += [\"love\", \"hate\", \"like\", \"dislike\", \"sad\", \"happy\", \"angry\", \"mad\", \"annoyed\", \"excited\", \"bored\", \"scared\", \"fear\", \"afraid\", \"surprised\", \"surprise\", \"disgusted\", \"disgust\", \"shocked\", \"shock\", \"confused\", \"confuse\", \"confusing\", \"confusion\", \"depressed\", \"depress\", \"depressing\", \"depression\", \"anxious\", \"anxiety\", \"anxious\", \"anxiously\"]\n",
        "  features += [\"damn\", \"omg\", \";-)\", \"<3\", \"grrr\", \"hehehe\", \"hehe\", \":p\", \":P\" ]\n",
        "  features += [\":)\", \":(\", \":D\", \":P\", \":/\", \":|\", \":O\", \":S\", \":*\", \":'(\"]\n",
        "  features += [\"haha\", \"hahaha\", \"lol\", \"lmao\", \"rofl\", \"O:\", \":O\", \"o:\", \":o\", \"wtf\"]\n",
        "\n",
        "\n",
        "  return {token: token in custom for token in features}\n"
      ],
      "metadata": {
        "id": "w9CBUttQKEzs"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conduct the following tasks:\n",
        "*   Train two Naive Bayes classifiers on the **Emotion + whQuestions** training set: one that uses the `get_word_features` function and another using `get_custom_features`.\n",
        "*   Evaluate each classifier on the test set. How accurate are they? Which one is better?\n",
        "*   What are the 20 most informative features for each classifier?\n"
      ],
      "metadata": {
        "id": "KxGa5GS1J3aG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your solution here\n",
        "\n",
        "# Prepare training sets with word features and custom features\n",
        "training_word_features = [(get_word_features(post), label) for post, label in training_set]\n",
        "training_custom_features = [(get_custom_features(post), label) for post, label in training_set]\n",
        "\n",
        "# Prepare test sets with word features and custom features\n",
        "test_word_features = [(get_word_features(post), label) for post, label in test_set]\n",
        "test_custom_features = [(get_custom_features(post), label) for post, label in test_set]\n",
        "\n",
        "# Train Naive Bayes Classifiers\n",
        "word_classifier = nltk.NaiveBayesClassifier.train(training_word_features)\n",
        "custom_classifier = nltk.NaiveBayesClassifier.train(training_custom_features)\n",
        "\n",
        "# Evaluate classifiers\n",
        "word_accuracy = nltk.classify.accuracy(word_classifier, test_word_features)\n",
        "custom_accuracy = nltk.classify.accuracy(custom_classifier, test_custom_features)\n",
        "\n",
        "\n",
        "print(f\"Accuracy (word features): {word_accuracy}\")\n",
        "print(f\"Accuracy (custom features): {custom_accuracy}\")\n",
        "\n",
        "# Show most informative features\n",
        "word_classifier.show_most_informative_features(20)\n",
        "custom_classifier.show_most_informative_features(20)\n"
      ],
      "metadata": {
        "id": "nml7temlB4-o",
        "outputId": "b58d420e-b73f-4c03-d8ba-f4d738cba9cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (word features): 0.9734513274336283\n",
            "Accuracy (custom features): 0.9911504424778761\n",
            "Most Informative Features\n",
            "                    what = True           whQues : Emotio =    181.4 : 1.0\n",
            "                     how = True           whQues : Emotio =    118.1 : 1.0\n",
            "                      at = True           whQues : Emotio =     93.6 : 1.0\n",
            "                      do = True           whQues : Emotio =     58.7 : 1.0\n",
            "                      up = True           whQues : Emotio =     37.6 : 1.0\n",
            "                     who = True           whQues : Emotio =     34.2 : 1.0\n",
            "                      in = True           whQues : Emotio =     34.0 : 1.0\n",
            "                     and = True           whQues : Emotio =     32.3 : 1.0\n",
            "                     you = True           whQues : Emotio =     26.5 : 1.0\n",
            "                    that = True           whQues : Emotio =     23.1 : 1.0\n",
            "                      is = True           whQues : Emotio =     20.7 : 1.0\n",
            "                    from = True           whQues : Emotio =     19.1 : 1.0\n",
            "                    lmao = True           Emotio : whQues =     18.1 : 1.0\n",
            "                      so = True           whQues : Emotio =     17.2 : 1.0\n",
            "                     the = True           whQues : Emotio =     17.2 : 1.0\n",
            "                     all = True           whQues : Emotio =     15.4 : 1.0\n",
            "                      be = True           whQues : Emotio =     15.4 : 1.0\n",
            "                      hi = True           whQues : Emotio =     15.2 : 1.0\n",
            "                      on = True           whQues : Emotio =     13.9 : 1.0\n",
            "                     lol = True           Emotio : whQues =     13.4 : 1.0\n",
            "Most Informative Features\n",
            "                    what = True           whQues : Emotio =    181.4 : 1.0\n",
            "                     how = True           whQues : Emotio =    118.1 : 1.0\n",
            "                     who = True           whQues : Emotio =     34.2 : 1.0\n",
            "                    lmao = True           Emotio : whQues =     18.1 : 1.0\n",
            "                     lol = True           Emotio : whQues =     13.4 : 1.0\n",
            "                    haha = True           Emotio : whQues =     11.8 : 1.0\n",
            "                     omg = True           Emotio : whQues =      3.1 : 1.0\n",
            "                       ? = False          Emotio : whQues =      2.6 : 1.0\n",
            "                     lol = False          whQues : Emotio =      1.7 : 1.0\n",
            "                       ! = True           Emotio : whQues =      1.6 : 1.0\n",
            "                    what = False          Emotio : whQues =      1.5 : 1.0\n",
            "                     how = False          Emotio : whQues =      1.3 : 1.0\n",
            "                    grrr = True           whQues : Emotio =      1.2 : 1.0\n",
            "                     who = False          Emotio : whQues =      1.2 : 1.0\n",
            "                    lmao = False          whQues : Emotio =      1.1 : 1.0\n",
            "                     why = False          Emotio : whQues =      1.1 : 1.0\n",
            "                   where = False          Emotio : whQues =      1.1 : 1.0\n",
            "                    haha = False          whQues : Emotio =      1.1 : 1.0\n",
            "                      :) = False          whQues : Emotio =      1.0 : 1.0\n",
            "                  hahaha = False          whQues : Emotio =      1.0 : 1.0\n"
          ]
        }
      ]
    }
  ]
}