{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/giorgiosld/Natural-Language-Processing/blob/main/labs/lab5/T_725_Lab05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3IOeFoye2P8"
      },
      "source": [
        "# T-725 Natural Language Processing: Lab 5\n",
        "In today's lab, we will be working with neural networks, using GRUs and Transformers for text generation.\n",
        "\n",
        "To begin with, do the following:\n",
        "* Select `\"File\" > \"Save a copy in Drive\"` to create a local copy of this notebook that you can edit.\n",
        "* **Select `\"Runtime\" > \"Change runtime type\"`, and make sure that you have \"Hardware accelerator\" set to \"GPU\"**\n",
        "* Select `\"Runtime\" > \"Run all\"` to run the code in this notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7ElxTOtl6UQ"
      },
      "source": [
        "import os\n",
        "import warnings\n",
        "\n",
        "# Suppress some warnings from TensorFlow about deprecated functions\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ayz3HiU7JvCF"
      },
      "source": [
        "## Generating text with neural networks\n",
        "Let's create a neural language model and use it to generate some text. This time, we will use character embeddings rather than word embeddings. They are created in exactly the same way, and are often used together in neural network-based models. One benefit of using character embeddings is that we can generate words that our model has never seen before.\n",
        "\n",
        "The model takes as input a sequence of characters and predicts which character is most likely to follow. We will generate text by repeatedly predicting and appending the next character to a string. First, however, we need some text to train it on.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PN7I_djD91Py"
      },
      "source": [
        "# Based on the following tutorial:\n",
        "# https://www.tensorflow.org/tutorials/text/text_generation\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "\n",
        "# Let's download some text by Shakespeare to train our model\n",
        "url = 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt'\n",
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', url)\n",
        "\n",
        "with open(path_to_file, encoding='utf-8') as f:\n",
        "  shakespeare = f.read()\n",
        "\n",
        "print(\"First 250 characters:\")\n",
        "print(shakespeare[:250])\n",
        "\n",
        "print (\"Length of text: {:,} characters\".format(len(shakespeare)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45JqVDxqtf1o"
      },
      "source": [
        "Now we can create training examples for our model. Each example will be a pair of strings: one input string containing 100 characters, and a target string that is one character ahead. For example, the first pair we create is:\n",
        "\n",
        "**Input string**:  `'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'`\n",
        "\n",
        "**Target string**: `'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '`\n",
        "\n",
        "However, before we can start training, we need to convert our text into a list of integers, where each integer represents a different character. For example, \"First Citizen\" becomes:\n",
        "\n",
        "```\n",
        "Character:   F   i   r   s   t      C   i   t   i   z   e   n\n",
        "Integer:   [18, 47, 56, 57, 58, 1, 15, 47, 58, 47, 64, 43, 52]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyper-parameters:\n",
        "\n",
        "BATCH_SIZE = 64  # Batch size\n",
        "BUFFER_SIZE = 10000  # Buffer size to shuffle the dataset\n",
        "SEQUENCE_LENGTH = 100  # Length of input sequence\n",
        "EMBEDDING_DIMENSION = 65  # Embedding dimension\n",
        "RNN_UNITS = 1024  # Number of RNN units"
      ],
      "metadata": {
        "id": "g2DuG2j6KIwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWPZjI0xHJ44"
      },
      "source": [
        "def split_input_target(chunk):\n",
        "  # Create (input_string, output_string) pairs\n",
        "  input_text = chunk[:-1]\n",
        "  target_text = chunk[1:]\n",
        "  return input_text, target_text\n",
        "\n",
        "def prepare_text(text):\n",
        "  # The unique characters in the file\n",
        "  vocab = sorted(set(text))\n",
        "  print ('{} unique characters'.format(len(vocab)))\n",
        "\n",
        "  # Creating a mapping from unique characters to indices\n",
        "  char_map = {\n",
        "      'char_to_index': {char: index for index, char in enumerate(vocab)},\n",
        "      'index_to_char': np.array(vocab)\n",
        "  }\n",
        "\n",
        "  text_as_int = np.array([char_map['char_to_index'][c] for c in text])\n",
        "\n",
        "  # The maximum length sentence we want for a single input in characters\n",
        "  seq_length = SEQUENCE_LENGTH\n",
        "  examples_per_epoch = len(text) // (seq_length+1)\n",
        "\n",
        "  # Create training examples / targets\n",
        "  char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "  sequences = char_dataset.batch(seq_length + 1, drop_remainder=True)\n",
        "  dataset = sequences.map(split_input_target)\n",
        "\n",
        "  # (TF data is designed to work with possibly infinite sequences,\n",
        "  # so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "  # it maintains a buffer in which it shuffles elements).\n",
        "  dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "  return dataset, vocab, examples_per_epoch, char_map"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPGoaE2TDnX9"
      },
      "source": [
        "Now we can create and train the neural network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5bOTe1hDqtY"
      },
      "source": [
        "import os\n",
        "\n",
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "\n",
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "      tf.keras.layers.Embedding(vocab_size,\n",
        "                                embedding_dim),\n",
        "      tf.keras.layers.GRU(rnn_units,\n",
        "                          return_sequences=True,\n",
        "                          recurrent_initializer='glorot_uniform',\n",
        "                          stateful=True),\n",
        "      tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(text, epochs=3):\n",
        "  dataset, vocab, examples_per_epoch, char_map = prepare_text(text)\n",
        "\n",
        "  train_model = build_model(len(vocab), EMBEDDING_DIMENSION, RNN_UNITS, BATCH_SIZE)\n",
        "  train_model.compile(optimizer='adam', loss=loss)\n",
        "\n",
        "  train_model.fit(dataset, epochs=epochs)\n",
        "\n",
        "  pred_model = build_model(len(vocab), EMBEDDING_DIMENSION, RNN_UNITS, batch_size=1)\n",
        "  pred_model.build(input_shape=(1, 100))\n",
        "  pred_model.set_weights(train_model.get_weights())\n",
        "\n",
        "  return pred_model, char_map"
      ],
      "metadata": {
        "id": "bj3VpwKNJDUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shakes_model, shakes_chars = create_model(shakespeare, epochs=3)"
      ],
      "metadata": {
        "id": "WkuDhT7eGkzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_R1efC2eKH1"
      },
      "source": [
        "Now that we've trained our model, we can finally use it to generate some text. The following function takes a model and a string as input, and continually predicts and appends the next character to the string until it becomes 1,000 characters long."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhVcs5ny-urX"
      },
      "source": [
        "def generate_text(model, char_map, start_string, temperature=1.0):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  if not start_string:\n",
        "    print(\"start_string can't be empty\")\n",
        "    return \"\"\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 1000\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [char_map['char_to_index'][s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Here batch size == 1\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a categorical distribution to predict the character returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # We pass the predicted word as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(char_map['index_to_char'][predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kS0UlqVbhOwa"
      },
      "source": [
        "Let's generate some text!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YOnJYAn-upC"
      },
      "source": [
        "print(generate_text(shakes_model, shakes_chars, \"ROMEO: \", temperature=1.0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-n5WwsuZe0B6"
      },
      "source": [
        "# Assignment\n",
        "Answer the following questions and hand in your solution in Canvas before 23:59 on Friday, September 27th. Remember to save your file before uploading it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoKGONy4fSl3"
      },
      "source": [
        "## Question 1\n",
        "The `temperature` parameter of `generate_text()`, defined earlier in the notebook, controls how predictable the generated text will be. The lower the temperature, the more the function will tend to append the most likely character (according to the model's prediction). A higher temperature introduces some randomness, leading to more unpredictable text.\n",
        "\n",
        "The text we generated above used a temperature of 1.0. Try generating more text using the Shakespeare model:\n",
        "\n",
        "(a) once using a temperature of 0.2 and\n",
        "\n",
        "(b) again using a temperature of 0.8\n",
        "\n",
        "and describe the difference."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XsSNVxKEeGZb"
      },
      "source": [
        "# Your solution here\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hauhMD0gfV2o"
      },
      "source": [
        "## Question 2\n",
        "NLTK's `names` corpus contains a list of approximately 8,000 English names. Train a new model on `names_raw` for at least 20 epochs using the `create_model(text, epochs=n)` function defined earlier. Use the trained model to generate a list of names (with the `generate_text` function defined earlier), starting with your own first name. Your name should not contain any non-English characters, and should end with an `\\n`.\n",
        "\n",
        "Print out the names that do not appear in the training data.\n",
        "\n",
        "(a) Do you get any actual names (or at least names that sound plausible)?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEOIqRE9fWTF"
      },
      "source": [
        "# Don't modify this code cell\n",
        "import nltk\n",
        "from nltk.corpus import names\n",
        "nltk.download('names')\n",
        "\n",
        "# Print out a few examples\n",
        "names_raw = names.raw()\n",
        "names_unique = set(names_raw.split())\n",
        "names_raw = \"\\n\".join(names_unique)\n",
        "print(names_raw.splitlines()[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7Y8qE1tYR1n"
      },
      "source": [
        "# Your solution here\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Question 3\n",
        "The size of the model can make a difference when it comes to performance. Create a new model that has twice the number of hidden units as the previous model and double the size of the embeddings.\n",
        "\n",
        "(a) How does the performance change?\n",
        "\n",
        "(b) What happens if you decrease these parameters?"
      ],
      "metadata": {
        "id": "64h0OI83sXJE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your solution here\n"
      ],
      "metadata": {
        "id": "Myk6_IUitYjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 4\n",
        "Transformer large language models can also generate text. The following code imports a pretrained GPT-2 model from Huggingface's Transformer library. This model can then be used directly to generate text, given a prompt as context. Alter the prompt to have the transformer model (GPT-2) generate an engaging story beginning using one of the following story starters:\n",
        "\n",
        "\n",
        "*   It was the day the moon fell.\n",
        "*   Am I in heaven?  What happened to me?\n",
        "*   Wandering through the graveyard it felt like something was watching me.\n",
        "*   Three of us.  We were the only ones left, the only ones to make it to the island.\n",
        "\n",
        "There are several different methods to choose from to generate the text (as seen in the commented out lines below). Try out the different methods and play with the parameters. This [blogpost](https://huggingface.co/blog/how-to-generate) explains their differences.\n",
        "\n",
        "(a) Which method has the best performance?\n",
        "\n",
        "(b) Can GPT-2 generate Shakespere?"
      ],
      "metadata": {
        "id": "2FI-1ldhn0SE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Uncomment if transformers is not installed\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "GE-xTvvYvHAy",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Do not modify this code\n",
        "# https://huggingface.co/docs/transformers/main_classes/text_generation\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "gpt2_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")"
      ],
      "metadata": {
        "id": "2_ZMgywnnziH",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Do not modify this code\n",
        "\n",
        "prompt = \"Today I believe we can finally\"\n",
        "\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "\n",
        "outputs = gpt2_model.generate(input_ids, max_length=100) # Greedy search\n",
        "#outputs = gpt2_model.generate(input_ids, max_length=100, num_beams=5, no_repeat_ngram_size=3, early_stopping=True) # Beam search\n",
        "#outputs = gpt2_model.generate(input_ids, do_sample=True, max_length=100, top_k=0, temperature=0.7) # Sampling\n",
        "#outputs = gpt2_model.generate(input_ids, do_sample=True, max_length=100, top_k=50) # Top-k\n",
        "#outputs = gpt2_model.generate(input_ids, do_sample=True, max_length=100, top_k=50, top_p=0.92) # Top-p\n",
        "\n",
        "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "LYFHsFPJMKbz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Your answer here\n"
      ],
      "metadata": {
        "id": "hnH32YYKraN4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}