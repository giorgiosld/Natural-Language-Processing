{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/giorgiosld/Natural-Language-Processing/blob/main/labs/lab5/T_725_Lab05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3IOeFoye2P8"
      },
      "source": [
        "# T-725 Natural Language Processing: Lab 5\n",
        "In today's lab, we will be working with neural networks, using GRUs and Transformers for text generation.\n",
        "\n",
        "To begin with, do the following:\n",
        "* Select `\"File\" > \"Save a copy in Drive\"` to create a local copy of this notebook that you can edit.\n",
        "* **Select `\"Runtime\" > \"Change runtime type\"`, and make sure that you have \"Hardware accelerator\" set to \"GPU\"**\n",
        "* Select `\"Runtime\" > \"Run all\"` to run the code in this notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7ElxTOtl6UQ"
      },
      "source": [
        "import os\n",
        "import warnings\n",
        "\n",
        "# Suppress some warnings from TensorFlow about deprecated functions\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ayz3HiU7JvCF"
      },
      "source": [
        "## Generating text with neural networks\n",
        "Let's create a neural language model and use it to generate some text. This time, we will use character embeddings rather than word embeddings. They are created in exactly the same way, and are often used together in neural network-based models. One benefit of using character embeddings is that we can generate words that our model has never seen before.\n",
        "\n",
        "The model takes as input a sequence of characters and predicts which character is most likely to follow. We will generate text by repeatedly predicting and appending the next character to a string. First, however, we need some text to train it on.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PN7I_djD91Py",
        "outputId": "da04b33e-86f2-4f40-a2ec-c7aac8eb26c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Based on the following tutorial:\n",
        "# https://www.tensorflow.org/tutorials/text/text_generation\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "\n",
        "# Let's download some text by Shakespeare to train our model\n",
        "url = 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt'\n",
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', url)\n",
        "\n",
        "with open(path_to_file, encoding='utf-8') as f:\n",
        "  shakespeare = f.read()\n",
        "\n",
        "print(\"First 250 characters:\")\n",
        "print(shakespeare[:250])\n",
        "\n",
        "print (\"Length of text: {:,} characters\".format(len(shakespeare)))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "\u001b[1m1115394/1115394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1us/step\n",
            "First 250 characters:\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "Length of text: 1,115,394 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45JqVDxqtf1o"
      },
      "source": [
        "Now we can create training examples for our model. Each example will be a pair of strings: one input string containing 100 characters, and a target string that is one character ahead. For example, the first pair we create is:\n",
        "\n",
        "**Input string**:  `'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'`\n",
        "\n",
        "**Target string**: `'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '`\n",
        "\n",
        "However, before we can start training, we need to convert our text into a list of integers, where each integer represents a different character. For example, \"First Citizen\" becomes:\n",
        "\n",
        "```\n",
        "Character:   F   i   r   s   t      C   i   t   i   z   e   n\n",
        "Integer:   [18, 47, 56, 57, 58, 1, 15, 47, 58, 47, 64, 43, 52]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyper-parameters:\n",
        "\n",
        "BATCH_SIZE = 64  # Batch size\n",
        "BUFFER_SIZE = 10000  # Buffer size to shuffle the dataset\n",
        "SEQUENCE_LENGTH = 100  # Length of input sequence\n",
        "EMBEDDING_DIMENSION = 65  # Embedding dimension\n",
        "RNN_UNITS = 1024  # Number of RNN units"
      ],
      "metadata": {
        "id": "g2DuG2j6KIwG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWPZjI0xHJ44"
      },
      "source": [
        "def split_input_target(chunk):\n",
        "  # Create (input_string, output_string) pairs\n",
        "  input_text = chunk[:-1]\n",
        "  target_text = chunk[1:]\n",
        "  return input_text, target_text\n",
        "\n",
        "def prepare_text(text):\n",
        "  # The unique characters in the file\n",
        "  vocab = sorted(set(text))\n",
        "  print ('{} unique characters'.format(len(vocab)))\n",
        "\n",
        "  # Creating a mapping from unique characters to indices\n",
        "  char_map = {\n",
        "      'char_to_index': {char: index for index, char in enumerate(vocab)},\n",
        "      'index_to_char': np.array(vocab)\n",
        "  }\n",
        "\n",
        "  text_as_int = np.array([char_map['char_to_index'][c] for c in text])\n",
        "\n",
        "  # The maximum length sentence we want for a single input in characters\n",
        "  seq_length = SEQUENCE_LENGTH\n",
        "  examples_per_epoch = len(text) // (seq_length+1)\n",
        "\n",
        "  # Create training examples / targets\n",
        "  char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "  sequences = char_dataset.batch(seq_length + 1, drop_remainder=True)\n",
        "  dataset = sequences.map(split_input_target)\n",
        "\n",
        "  # (TF data is designed to work with possibly infinite sequences,\n",
        "  # so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "  # it maintains a buffer in which it shuffles elements).\n",
        "  dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "  return dataset, vocab, examples_per_epoch, char_map"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPGoaE2TDnX9"
      },
      "source": [
        "Now we can create and train the neural network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5bOTe1hDqtY"
      },
      "source": [
        "import os\n",
        "\n",
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "\n",
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "      tf.keras.layers.Embedding(vocab_size,\n",
        "                                embedding_dim),\n",
        "      tf.keras.layers.GRU(rnn_units,\n",
        "                          return_sequences=True,\n",
        "                          recurrent_initializer='glorot_uniform',\n",
        "                          stateful=True),\n",
        "      tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "\n",
        "  return model"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(text, epochs=3):\n",
        "  dataset, vocab, examples_per_epoch, char_map = prepare_text(text)\n",
        "\n",
        "  train_model = build_model(len(vocab), EMBEDDING_DIMENSION, RNN_UNITS, BATCH_SIZE)\n",
        "  train_model.compile(optimizer='adam', loss=loss)\n",
        "\n",
        "  train_model.fit(dataset, epochs=epochs)\n",
        "\n",
        "  pred_model = build_model(len(vocab), EMBEDDING_DIMENSION, RNN_UNITS, batch_size=1)\n",
        "  pred_model.build(input_shape=(1, 100))\n",
        "  pred_model.set_weights(train_model.get_weights())\n",
        "\n",
        "  return pred_model, char_map"
      ],
      "metadata": {
        "id": "bj3VpwKNJDUd"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shakes_model, shakes_chars = create_model(shakespeare, epochs=3)"
      ],
      "metadata": {
        "id": "WkuDhT7eGkzd",
        "outputId": "42393964-82db-4961-ac62-21ba649dd452",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65 unique characters\n",
            "Epoch 1/3\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 53ms/step - loss: 3.2147\n",
            "Epoch 2/3\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 48ms/step - loss: 2.0777\n",
            "Epoch 3/3\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 1.7864\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_R1efC2eKH1"
      },
      "source": [
        "Now that we've trained our model, we can finally use it to generate some text. The following function takes a model and a string as input, and continually predicts and appends the next character to the string until it becomes 1,000 characters long."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhVcs5ny-urX"
      },
      "source": [
        "def generate_text(model, char_map, start_string, temperature=1.0):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  if not start_string:\n",
        "    print(\"start_string can't be empty\")\n",
        "    return \"\"\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 1000\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [char_map['char_to_index'][s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Here batch size == 1\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a categorical distribution to predict the character returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # We pass the predicted word as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(char_map['index_to_char'][predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kS0UlqVbhOwa"
      },
      "source": [
        "Let's generate some text!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YOnJYAn-upC",
        "outputId": "625a6dc5-bb4d-4dd7-ee03-b3538aeaede0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(generate_text(shakes_model, shakes_chars, \"ROMEO: \", temperature=1.0))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO: Eveating soul;\n",
            "And, you; pretagution; anitherous latter un,\n",
            "Hast onew if he'd impnation a myselff-I been it is hall me.\n",
            "\n",
            "HARTING:\n",
            "I'll buties and roseingerngle pried's whomemone have art the Lounds.\n",
            "\n",
            "ISABELLA:\n",
            "Your whilst I shall tale! I maye the criem,\n",
            "I break to brake ho beat so by theme innone,\n",
            "The noble of Fad you say he tow his hands of ary.\n",
            "\n",
            "AUFlIRS:\n",
            "'This worden by that men's 'ang of his world,\n",
            "And cannot in me ROMUMEN:\n",
            "Who, gonsue; in?-Gullly lasigale geed\n",
            "The ittreen, by tent hem\n",
            "And, you to churger beswer.\n",
            "\n",
            "JROUPERLE:\n",
            "Inall, net-not nothing toon him: has a fer a tooce:\n",
            "To no be be objementing grecious Composh of his poor lettes,\n",
            "honour'd ane to your thouse to One,\n",
            "This soulch ann to snacious of.\n",
            "\n",
            "WARMINY:\n",
            "Some it ba tid Youkt to-\n",
            "Wo whose scepfurd.\n",
            "\n",
            "ESTORIONA:\n",
            "'Tis nefe them.'\n",
            "Of the sutether some hore canst the everty mish dram, porshy,\n",
            "Or the consommonafles cail;\n",
            "You like maven blood that seld-ind gaves a conor\n",
            "He rmoms a wingle wit: he sensit in\n",
            "Giglieve of sopprick, and, \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-n5WwsuZe0B6"
      },
      "source": [
        "# Assignment\n",
        "Answer the following questions and hand in your solution in Canvas before 23:59 on Friday, September 27th. Remember to save your file before uploading it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoKGONy4fSl3"
      },
      "source": [
        "## Question 1\n",
        "The `temperature` parameter of `generate_text()`, defined earlier in the notebook, controls how predictable the generated text will be. The lower the temperature, the more the function will tend to append the most likely character (according to the model's prediction). A higher temperature introduces some randomness, leading to more unpredictable text.\n",
        "\n",
        "The text we generated above used a temperature of 1.0. Try generating more text using the Shakespeare model:\n",
        "\n",
        "(a) once using a temperature of 0.2 and\n",
        "\n",
        "(b) again using a temperature of 0.8\n",
        "\n",
        "and describe the difference."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XsSNVxKEeGZb",
        "outputId": "c01021bd-f328-443f-b1ce-12cb00230c44",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Your solution here\n",
        "print(\"Generated text with temperature of 0.8\")\n",
        "print(generate_text(shakes_model, shakes_chars, \"ROMEO: \", temperature=0.8))\n",
        "print(\"Generated text with temperature of 0.2\")\n",
        "print(generate_text(shakes_model, shakes_chars, \"ROMEO: \", temperature=0.2))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated text with temperature of 0.8\n",
            "ROMEO: my leader, and it, both I faip for the matter?\n",
            "\n",
            "PLORY OF GAGH:\n",
            "Pencereate that so lobed in hairated store o' to the consule may\n",
            "To send the fairing; and scine a crovented-age,\n",
            "I am sidess were to benith thee, with that do I thee were.\n",
            "\n",
            "First Censting upon the stright sporn,\n",
            "Enter for the bony of the playsall heaven.\n",
            "Hasting injuther, no sol of his with to your hath,\n",
            "When cannot should to seaven coursey sound'd, him, they,\n",
            "Take I day not that it, and handstake black it.\n",
            "\n",
            "HARTICK:\n",
            "To sause to be a monall.\n",
            "\n",
            "ROMEO:\n",
            "Well, gray to rest in marth in this hat's frater his gentle,\n",
            "To and not the cittle and my cault\n",
            "As was for therefore hast to at such anstand\n",
            "Hast demestarse unstracions with not be rest you.\n",
            "\n",
            "LUCIO:\n",
            "I take come brother so me senet in the streatht;\n",
            "Bid you sweet him it mean not from the follows rigot,\n",
            "And that I hear indurk'd to the cknot his flive,\n",
            "Allostes he is be hate his flatter bear me,\n",
            "I that agand me; is the wall,\n",
            "thes her by the proved and flather was are toupent.\n",
            "\n",
            "LEONT\n",
            "Generated text with temperature of 0.2\n",
            "ROMEO: the king of the comes to be to be a batter the compost the best of the counter to be a man hath the conself\n",
            "That is the parther to the will of the coust the comes and my best and the conself to be to be sonerent to see the prople and the consent the compost the comport the content to be to be to be contented to be here.\n",
            "\n",
            "LUCIO:\n",
            "I may be heard the king of the counter to have heart the conself and the consent to the will of the courter to be to be to be to be hath heart,\n",
            "That will the consent the consul to the wind of his forth and his father's seath,\n",
            "That is the poor of the consent the hand and thee the courter to be sone to be the consent to be a bast and the conself\n",
            "To be to be the content to the wings of the compont\n",
            "To shall be the constant to be the content the forth the world have your hands and heart the conself\n",
            "To have so shall be to be the consent to be be to be content the with the conself\n",
            "That we will not the conself and the conself\n",
            "That with the content the consent to the con\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since an high temperature produce more randomness than the lower ones, using the temperature of 0.8 we have shorter sentences than the other generates using as value 0.2. Seems to be less grammatical errors with lower temperature (e.g. 0.2) compared to higher one (e.g. 0.8)."
      ],
      "metadata": {
        "id": "pJPgEU30w1_8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hauhMD0gfV2o"
      },
      "source": [
        "## Question 2\n",
        "NLTK's `names` corpus contains a list of approximately 8,000 English names. Train a new model on `names_raw` for at least 20 epochs using the `create_model(text, epochs=n)` function defined earlier. Use the trained model to generate a list of names (with the `generate_text` function defined earlier), starting with your own first name. Your name should not contain any non-English characters, and should end with an `\\n`.\n",
        "\n",
        "Print out the names that do not appear in the training data.\n",
        "\n",
        "(a) Do you get any actual names (or at least names that sound plausible)?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEOIqRE9fWTF",
        "outputId": "83b509b9-3fd2-4c90-8e90-1ff8de7c6c18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Don't modify this code cell\n",
        "import nltk\n",
        "from nltk.corpus import names\n",
        "nltk.download('names')\n",
        "\n",
        "# Print out a few examples\n",
        "names_raw = names.raw()\n",
        "names_unique = set(names_raw.split())\n",
        "names_raw = \"\\n\".join(names_unique)\n",
        "print(names_raw.splitlines()[:5])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Valina', 'Darsie', 'Verla', 'Moria', 'Adey']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package names to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/names.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7Y8qE1tYR1n",
        "outputId": "986596cc-1880-43d6-a19d-c32392c0de19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Your solution here\n",
        "names_model, names_chars = create_model(names_raw, epochs=20)\n",
        "print(generate_text(names_model, names_chars, \"Giorgio\\n\", temperature=0.2))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "55 unique characters\n",
            "Epoch 1/20\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 59ms/step - loss: 3.9299\n",
            "Epoch 2/20\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - loss: 3.7486\n",
            "Epoch 3/20\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - loss: 3.1864\n",
            "Epoch 4/20\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 2.8473\n",
            "Epoch 5/20\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 2.5858\n",
            "Epoch 6/20\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 2.4778\n",
            "Epoch 7/20\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - loss: 2.4281\n",
            "Epoch 8/20\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 2.3788\n",
            "Epoch 9/20\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 2.3520\n",
            "Epoch 10/20\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 2.3166\n",
            "Epoch 11/20\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 2.2899\n",
            "Epoch 12/20\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - loss: 2.2730\n",
            "Epoch 13/20\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 2.2542\n",
            "Epoch 14/20\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 2.2400\n",
            "Epoch 15/20\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 2.2253\n",
            "Epoch 16/20\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 2.2118\n",
            "Epoch 17/20\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - loss: 2.2000\n",
            "Epoch 18/20\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 2.1819\n",
            "Epoch 19/20\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - loss: 2.1719\n",
            "Epoch 20/20\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - loss: 2.1671\n",
            "Giorgio\n",
            "Maris\n",
            "Allina\n",
            "Sherina\n",
            "Alane\n",
            "Anna\n",
            "Garina\n",
            "Dorina\n",
            "Caris\n",
            "Linna\n",
            "Anna\n",
            "Daria\n",
            "Lorina\n",
            "Anda\n",
            "Dorina\n",
            "Ganna\n",
            "Jona\n",
            "Anna\n",
            "Maria\n",
            "Allen\n",
            "Lorie\n",
            "Alina\n",
            "Lonna\n",
            "Charis\n",
            "Andie\n",
            "Annie\n",
            "Dorina\n",
            "Dorina\n",
            "Allen\n",
            "Allina\n",
            "Christa\n",
            "Allina\n",
            "Maris\n",
            "Sherina\n",
            "Jonna\n",
            "Jorina\n",
            "Ellina\n",
            "Jonna\n",
            "Carrina\n",
            "Sheri\n",
            "Allen\n",
            "Allina\n",
            "Dorina\n",
            "Darine\n",
            "Allina\n",
            "Maris\n",
            "Andorina\n",
            "Marie\n",
            "Anna\n",
            "Darina\n",
            "Annela\n",
            "Allis\n",
            "Karisa\n",
            "Ellina\n",
            "Dorina\n",
            "Branda\n",
            "Allin\n",
            "Allen\n",
            "Annel\n",
            "Morina\n",
            "Torina\n",
            "Marie\n",
            "Alina\n",
            "Sherta\n",
            "Carisa\n",
            "Allen\n",
            "Alena\n",
            "Marisa\n",
            "Allan\n",
            "Arris\n",
            "Andie\n",
            "Alina\n",
            "Maris\n",
            "Alana\n",
            "Carista\n",
            "Sharine\n",
            "Garina\n",
            "Maris\n",
            "Maris\n",
            "Alanna\n",
            "Jona\n",
            "Rona\n",
            "Maria\n",
            "Marie\n",
            "Aranna\n",
            "Alela\n",
            "Ellina\n",
            "Sherina\n",
            "Rona\n",
            "Danda\n",
            "Aris\n",
            "Arina\n",
            "Doria\n",
            "Maris\n",
            "Anne\n",
            "Allen\n",
            "Marila\n",
            "Daria\n",
            "Rona\n",
            "Carila\n",
            "Allina\n",
            "Arina\n",
            "Charis\n",
            "Maris\n",
            "Ander\n",
            "Allina\n",
            "Gerina\n",
            "Caris\n",
            "Allie\n",
            "Alanna\n",
            "Karie\n",
            "Arina\n",
            "Grina\n",
            "Darina\n",
            "Alia\n",
            "Allina\n",
            "Ellina\n",
            "Marie\n",
            "Annelle\n",
            "Alan\n",
            "Alanna\n",
            "Marina\n",
            "Charie\n",
            "Maris\n",
            "Allina\n",
            "Ellina\n",
            "Dorina\n",
            "Danne\n",
            "Torrin\n",
            "Andella\n",
            "Doria\n",
            "Sharina\n",
            "Ellina\n",
            "Tarina\n",
            "Ellen\n",
            "Lorina\n",
            "Alena\n",
            "Rona\n",
            "Ronne\n",
            "Adrina\n",
            "Caris\n",
            "Anna\n",
            "Allina\n",
            "Ellina\n",
            "Dorina\n",
            "Rona\n",
            "Sharina\n",
            "Allan\n",
            "Alanda\n",
            "Marie\n",
            "Alan\n",
            "Alisa\n",
            "Marisa\n",
            "Alena\n",
            "Ellina\n",
            "Norie\n",
            "Alis\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, I've got \"Marie\" like actual name using a temperature of 0.2, using higher ones the names that I've got has at the end a sound plausible"
      ],
      "metadata": {
        "id": "2sKDadEe0e4Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Question 3\n",
        "The size of the model can make a difference when it comes to performance. Create a new model that has twice the number of hidden units as the previous model and double the size of the embeddings.\n",
        "\n",
        "(a) How does the performance change?\n",
        "\n",
        "(b) What happens if you decrease these parameters?"
      ],
      "metadata": {
        "id": "64h0OI83sXJE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your solution here\n",
        "def create_powerful_model(text, epochs=3):\n",
        "    dataset, vocab, examples_per_epoch, char_map = prepare_text(text)\n",
        "\n",
        "    train_model = build_model(len(vocab), EMBEDDING_DIMENSION//2, RNN_UNITS//2, BATCH_SIZE)\n",
        "    train_model.compile(optimizer='adam', loss=loss)\n",
        "\n",
        "    train_model.fit(dataset, epochs=epochs)\n",
        "\n",
        "    pred_model = build_model(len(vocab), EMBEDDING_DIMENSION//2, RNN_UNITS//2, batch_size=1)\n",
        "    pred_model.build(input_shape=(1,100))\n",
        "    pred_model.set_weights(train_model.get_weights())\n",
        "\n",
        "    return pred_model, char_map\n"
      ],
      "metadata": {
        "id": "Myk6_IUitYjk"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "names_model, names_chars = create_powerful_model(names_raw, epochs=20)\n",
        "print(generate_text(names_model, names_chars, \"Giorgio\\n\", temperature=1))"
      ],
      "metadata": {
        "id": "GlsX6jh825aK",
        "outputId": "f12d1f36-e447-4c67-aa99-d8a31630f519",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "55 unique characters\n",
            "Epoch 1/20\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 3.9064\n",
            "Epoch 2/20\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 3.3581\n",
            "Epoch 3/20\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 3.1538\n",
            "Epoch 4/20\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 3.0697\n",
            "Epoch 5/20\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 2.8968\n",
            "Epoch 6/20\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 2.6705\n",
            "Epoch 7/20\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 2.5269\n",
            "Epoch 8/20\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 2.4526\n",
            "Epoch 9/20\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 2.4140\n",
            "Epoch 10/20\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 2.3777\n",
            "Epoch 11/20\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 2.3538\n",
            "Epoch 12/20\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 2.3303\n",
            "Epoch 13/20\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 2.3145\n",
            "Epoch 14/20\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 2.2945\n",
            "Epoch 15/20\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 2.2750\n",
            "Epoch 16/20\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 2.2647\n",
            "Epoch 17/20\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 2.2550\n",
            "Epoch 18/20\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 2.2385\n",
            "Epoch 19/20\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 2.2301\n",
            "Epoch 20/20\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 2.2231\n",
            "Giorgio\n",
            "Marra\n",
            "Sameli\n",
            "Ridanta\n",
            "Rodorry\n",
            "Luloy\n",
            "Fer\n",
            "Ellethe\n",
            "Ary\n",
            "Banvye\n",
            "Ginar\n",
            "Stexs\n",
            "Roma\n",
            "Bibong\n",
            "Rovah-Kentofbe\n",
            "And\n",
            "Barbnila\n",
            "Dorinnu\n",
            "Perinik\n",
            "Sinak\n",
            "Brens\n",
            "Nezost\n",
            "Furn\n",
            "Matbiesn\n",
            "Filline\n",
            "Leanne\n",
            "Tellry\n",
            "Ennie\n",
            "Garballa\n",
            "Dary\n",
            "Farigel\n",
            "Lageinne\n",
            "Rilk\n",
            "Ag\n",
            "Amis\n",
            "Leiille\n",
            "Hee\n",
            "Agsem\n",
            "Soseluod\n",
            "Isella\n",
            "Nicgatt\n",
            "Doztilia\n",
            "Vin\n",
            "Tanelia\n",
            "Kanie\n",
            "Canm\n",
            "Marand\n",
            "Aphille\n",
            "Benny\n",
            "Tuahana\n",
            "Carita\n",
            "Nabella\n",
            "Banl\n",
            "Clen\n",
            "Ada\n",
            "Gomlye\n",
            "Rante\n",
            "Lindye\n",
            "Adithy\n",
            "Osgen\n",
            "Luctina\n",
            "Ans\n",
            "Pelran\n",
            "Tetsa\n",
            "Kisnie\n",
            "Morry\n",
            "Leonta\n",
            "Shaze\n",
            "Etmie\n",
            "Sassaonne\n",
            "Ethaste\n",
            "Anconn\n",
            "Vichelich\n",
            "Pinia\n",
            "Amera\n",
            "unn\n",
            "Anarana\n",
            "Vicop\n",
            "Itheld\n",
            "Legty\n",
            "Ortia\n",
            "Slari\n",
            "Melpoan\n",
            "Gaile\n",
            "Hator\n",
            "Ametihh\n",
            "Lamotins\n",
            "Veriest\n",
            "Olbab\n",
            "Forra\n",
            "Fristila\n",
            "Verrietu\n",
            "Yuciatte\n",
            "Nenna\n",
            "Caodrerte\n",
            "Bane\n",
            "Gurtty\n",
            "Elra\n",
            "Tilvicana\n",
            "Janein\n",
            "Fraynna\n",
            "Charitie\n",
            "Sharba\n",
            "Seolddr\n",
            "Cornin\n",
            "Tiann\n",
            "Ela\n",
            "Sestahia\n",
            "Sha\n",
            "Caabi\n",
            "Tand\n",
            "Corris\n",
            "Meadin\n",
            "Alan\n",
            "Katisa\n",
            "Ketph\n",
            "Dagigtan\n",
            "Alon\n",
            "Chetie\n",
            "Cerlyde\n",
            "Ttefle\n",
            "Mamola\n",
            "Alnet\n",
            "Maullen\n",
            "Harittie\n",
            "Wuris\n",
            "Yesl\n",
            "Adelle\n",
            "Bietpini\n",
            "Dainy\n",
            "Maryar\n",
            "Kest\n",
            "Istice\n",
            "Bvocpuin\n",
            "Jabea\n",
            "Drig\n",
            "Seshe\n",
            "Ishele\n",
            "Manner\n",
            "Deidar\n",
            "Agiohan\n",
            "Ilros\n",
            "Lyrta\n",
            "Sirga\n",
            "Feynia\n",
            "Leimianne\n",
            "Jebel\n",
            "Baze\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With a model that has twice the number of hidden units and double the size of embedding the model has an improvement of performance even with 0.2 and with 1.0 as temperature."
      ],
      "metadata": {
        "id": "lPutv0va30tN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instead with a model with less hidden units and a lower size of embedding the model is faster to train It but has a great loss in perfomance"
      ],
      "metadata": {
        "id": "i9MqwOVk4S3f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 4\n",
        "Transformer large language models can also generate text. The following code imports a pretrained GPT-2 model from Huggingface's Transformer library. This model can then be used directly to generate text, given a prompt as context. Alter the prompt to have the transformer model (GPT-2) generate an engaging story beginning using one of the following story starters:\n",
        "\n",
        "\n",
        "*   It was the day the moon fell.\n",
        "*   Am I in heaven?  What happened to me?\n",
        "*   Wandering through the graveyard it felt like something was watching me.\n",
        "*   Three of us.  We were the only ones left, the only ones to make it to the island.\n",
        "\n",
        "There are several different methods to choose from to generate the text (as seen in the commented out lines below). Try out the different methods and play with the parameters. This [blogpost](https://huggingface.co/blog/how-to-generate) explains their differences.\n",
        "\n",
        "(a) Which method has the best performance?\n",
        "\n",
        "(b) Can GPT-2 generate Shakespere?"
      ],
      "metadata": {
        "id": "2FI-1ldhn0SE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Uncomment if transformers is not installed\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "GE-xTvvYvHAy",
        "collapsed": true,
        "outputId": "6f4dba41-b895-4d34-ebe5-d3498e97b297",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Do not modify this code\n",
        "# https://huggingface.co/docs/transformers/main_classes/text_generation\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "gpt2_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")"
      ],
      "metadata": {
        "id": "2_ZMgywnnziH",
        "collapsed": true,
        "outputId": "2db2f041-b925-40ba-bd92-863ab0c3c85e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Do not modify this code\n",
        "\n",
        "prompt = \"Today I believe we can finally\"\n",
        "\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "\n",
        "outputs = gpt2_model.generate(input_ids, max_length=100) # Greedy search\n",
        "#outputs = gpt2_model.generate(input_ids, max_length=100, num_beams=5, no_repeat_ngram_size=3, early_stopping=True) # Beam search\n",
        "#outputs = gpt2_model.generate(input_ids, do_sample=True, max_length=100, top_k=0, temperature=0.7) # Sampling\n",
        "#outputs = gpt2_model.generate(input_ids, do_sample=True, max_length=100, top_k=50) # Top-k\n",
        "#outputs = gpt2_model.generate(input_ids, do_sample=True, max_length=100, top_k=50, top_p=0.92) # Top-p\n",
        "\n",
        "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "LYFHsFPJMKbz",
        "outputId": "44bed2e5-7b52-415e-ab6a-14920f320fda",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Today I believe we can finally get to the point where we can make a difference in the lives of the people of the United States of America.\\n\\nI believe that we can make a difference in the lives of the people of the United States of America.\\n\\nI believe that we can make a difference in the lives of the people of the United States of America.\\n\\nI believe that we can make a difference in the lives of the people of the United States of America.\\n\\n']"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Your answer here\n",
        "prompt = \"Am I in heaven? What happened to me?\"\n",
        "\n",
        "input_ids = tokenizer(prompt, return_tensors='pt').input_ids\n",
        "#outputs = gpt2_model.generate(input_ids, max_length=100) # Greedy search\n",
        "#outputs = gpt2_model.generate(input_ids, max_length=100, num_beams=5, no_repeat_ngram_size=3, early_stopping=True) # Beam search\n",
        "#outputs = gpt2_model.generate(input_ids, do_sample=True, max_length=100, top_k=0, temperature=0.7) # Sampling\n",
        "outputs = gpt2_model.generate(input_ids, do_sample=True, max_length=100, top_k=50) # Top-k\n",
        "#outputs = gpt2_model.generate(input_ids, do_sample=True, max_length=100, top_k=50, top_p=0.92) # Top-p\n",
        "\n",
        "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "hnH32YYKraN4",
        "outputId": "0b8d9dfa-b72b-4423-8205-a27f3cc4f0f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"Am I in heaven? What happened to me?\\n\\nHe had a strong argument with me and his question should not have been asked. Since he was young there was no reason to question me or the answer to my question should not have been taken. I didn't dare ask it either.\\n\\nThe point of having a family for the long term would be to be a father. I knew it would be not good. It was a risk I would take even before I did what I felt\"]"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model trained with a combination of top_k and top_p outperform the other ones creating sentences more structured and meaningful."
      ],
      "metadata": {
        "id": "1dk183R7-Afy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = tokenizer(shakespeare[125:225], return_tensors='pt').input_ids\n",
        "\n",
        "outputs = gpt2_model.generate(input_ids, do_sample=True, max_length=100, top_k=50, top_p=0.92) # Top-p\n",
        "# outputs = gpt2_model.generate(input_ids, do_sample=True, max_length=100, top_k=50) # Top-k\n",
        "\n",
        "\n",
        "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "eY3Gz2l5-ZLc",
        "outputId": "02e5ca70-85d1-4322-b851-959793299929",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you know Caius Marcius is churlish.\\n\\nYou know that a noble was not a child of Caius Marcius until he was forced to become a slave?\\n\\nWell, I say, the first that you hear that is from Caius.\\n\\nAll:\\n\\nResolved. resolved.\\n\\nFirst Citizen:\\n\\n']"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seems that GPT-2 is not able to generate shakespeare neither using the top-k and the combinaion of top-k and top-p"
      ],
      "metadata": {
        "id": "TUT8PSiE-6Gr"
      }
    }
  ]
}