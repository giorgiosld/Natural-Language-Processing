{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/giorgiosld/Natural-Language-Processing/blob/main/labs/lab7/T_725_Lab07.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# T-725 Natural Language Processing: Lab 7\n",
        "In today's lab, we will be working with spaCy and Huggingface for a variety of tasks. We'll also learn how to use Gradio to quickly create convenient user interfaces.\n",
        "\n",
        "To begin with, do the following:\n",
        "* Select `\"File\" > \"Save a copy in Drive\"` to create a local copy of this notebook that you can edit.\n",
        "* **Select `\"Runtime\" > \"Change runtime type\"`, and make sure that you have \"Hardware accelerator\" set to \"GPU\"**\n",
        "* Select `\"Runtime\" > \"Run all\"` to run the code in this notebook."
      ],
      "metadata": {
        "id": "GVUWLZAzmLvK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## spaCy\n",
        "\n",
        "[spaCy](https://spacy.io) is a free, open-source library for advanced Natural Language Processing (NLP) in Python.\n",
        "\n",
        "spaCy is designed specifically for production use and helps you build applications that process and ‚Äúunderstand‚Äù large volumes of text. It can be used to build information extraction or natural language understanding systems, or to pre-process text for deep learning."
      ],
      "metadata": {
        "id": "vvRd4TzTm4uJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Features\n",
        "\n",
        "Name | Description\n",
        "---|---\n",
        "**Tokenization** | Segmenting text into words, punctuations marks etc.\n",
        "**Part-of-speech (POS) Tagging** | Assigning word types to tokens, like verb or noun.\n",
        "**Dependency Parsing** | Assigning syntactic dependency labels, describing the relations between individual tokens, like subject or object.\n",
        "**Lemmatization** |\tAssigning the base forms of words. For example, the lemma of ‚Äúwas‚Äù is ‚Äúbe‚Äù, and the lemma of ‚Äúrats‚Äù is ‚Äúrat‚Äù.\n",
        "**Sentence Boundary Detection (SBD)** |\tFinding and segmenting individual sentences.\n",
        "**Named Entity Recognition (NER)** | Labelling named ‚Äúreal-world‚Äù objects, like persons, companies or locations.\n",
        "**Entity Linking (EL)** | Disambiguating textual entities to unique identifiers in a knowledge base.\n",
        "**Similarity** | Comparing words, text spans and documents and how similar they are to each other.\n",
        "**Text Classification** | Assigning categories or labels to a whole document, or parts of a document.\n",
        "**Rule-based Matching** | Finding sequences of tokens based on their texts and linguistic annotations, similar to regular expressions.\n",
        "**Training** | Updating and improving a statistical model‚Äôs predictions.\n",
        "**Serialization** | Saving objects to files or byte strings."
      ],
      "metadata": {
        "id": "i_HhgHSAR5gI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trained Pipelines\n",
        "\n",
        "While some of spaCy‚Äôs features work independently, others require [trained pipelines](https://spacy.io/models) to be loaded, which enable spaCy to predict linguistic annotations ‚Äì for example, whether a word is a verb or a noun. A trained pipeline can consist of multiple components that use a statistical model trained on labeled data. spaCy currently offers trained pipelines for a variety of languages, which can be installed as individual Python modules."
      ],
      "metadata": {
        "id": "ryBLzQt2R0AD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summarization Example\n",
        "Let's take a look at some of the functionality of spaCy through the example of [automatic summarization](https://medium.com/luisfredgs/automatic-text-summarization-with-machine-learning-an-overview-68ded5717a25). There are two main types of summarization: extractive and abstractive. Extractive summarization selects a subset of sentences from the text to form a summary; abstractive summarization reorganizes the language in the text and adds novel words/phrases into the summary if necessary.\n",
        "\n",
        "For this example we'll be doing automatic [extractive summarization](https://medium.com/analytics-vidhya/text-summarization-using-spacy-ca4867c6b744)."
      ],
      "metadata": {
        "id": "yGGTyegyYyI_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First install spaCy:"
      ],
      "metadata": {
        "id": "fQ8XS7evav7g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NB: NLTK must be imported before installing spaCy\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "MZNmXnmkgDuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PkPnUi9HgSHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U spacy"
      ],
      "metadata": {
        "id": "C18jWsZaTwid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then import all necessary modules:"
      ],
      "metadata": {
        "id": "owNTDP4lazIk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from string import punctuation\n",
        "from collections import Counter\n",
        "from heapq import nlargest"
      ],
      "metadata": {
        "id": "kCxIVdDk0_cQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are many [other languages](https://spacy.io/usage/models) to choose from. Here we load the English language models:"
      ],
      "metadata": {
        "id": "Y_CC1uaSbAk0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "maUkAIjd1ASa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Choose some text to be summarized and store it in a variable:"
      ],
      "metadata": {
        "id": "b6Se2Tj0a6z9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "long_text = \"\"\"\n",
        "Machine learning (ML) is the scientific study of algorithms and statistical models\n",
        "that computer systems use to progressively improve their performance on a specific\n",
        "task. Machine learning algorithms build a mathematical model of sample data, known as\n",
        "‚Äútraining data‚Äù, in order to make predictions or decisions without being explicitly\n",
        "programmed to perform the task. Machine learning algorithms are used in the applications\n",
        "of email filtering, detection of network intruders, and computer vision, where it\n",
        "is infeasible to develop an algorithm of specific instructions for performing the task.\n",
        "Machine learning is closely related to computational statistics, which focuses on\n",
        "making predictions using computers. The study of mathematical optimization delivers\n",
        "methods, theory and application domains to the field of machine learning. Data mining\n",
        "is a field of study within machine learning and focuses on exploratory data analysis\n",
        "through unsupervised learning. In its application across business problems, machine\n",
        "learning is also referred to as predictive analytics.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "1UejJJP_1NM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pass the text to the `nlp` function:"
      ],
      "metadata": {
        "id": "psGxQfn6bS2z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(long_text)"
      ],
      "metadata": {
        "id": "tZ-X-fVX1Qlj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point, the text has been processed, i.e., tokenized, lemmatized, tagged with parts-of-speech, and parsed. A variety of lingustic features are accessbile via the `doc` object, e.g.:\n",
        "\n",
        "* Lemmas\n",
        "* Parts of speech\n",
        "* Dependency parse\n",
        "* Named entities\n",
        "* Chunks\n",
        "* Is alphabet character\n",
        "* Is capitalized\n",
        "* Is in the stop list\n",
        "\n",
        "The following will print out each of those bits of information for every token in the original text, one token per line:"
      ],
      "metadata": {
        "id": "s7oEufIlb31T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc:\n",
        "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
        "            token.shape_, token.is_alpha, token.is_stop)"
      ],
      "metadata": {
        "id": "CBCarRfubsOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we'll use this information to filter keywords from the original text.\n",
        "\n",
        "* Define the keywords list\n",
        "* Choose the parts-of-speech that are likely to be important ([pos tags in spaCy](https://spacy.io/usage/linguistic-features/#pos-tagging))\n",
        "* Skip tokens that are in the stop list\n",
        "* Add tokens that have the part-of-speech we care about to the keywords list"
      ],
      "metadata": {
        "id": "Q3NBen5aeQLb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keywords = []\n",
        "pos_tags = [\"PROPN\", \"ADJ\", \"NOUN\", \"VERB\"]\n",
        "for token in doc:\n",
        "  if token.is_stop:\n",
        "    continue\n",
        "  if token.pos_ in pos_tags:\n",
        "    keywords.append(token.text)"
      ],
      "metadata": {
        "id": "wHhJ4dUzebKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we calculate the frequency of each token using the `Counter` function and store it in `freq_words`.\n",
        "\n",
        "To view the top `n` most frequent words, the `most_common(n)` method can be used:"
      ],
      "metadata": {
        "id": "flCldV1PfwGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "freq_words = Counter(keywords)\n",
        "freq_words.most_common(5)"
      ],
      "metadata": {
        "id": "uudzeyqtfNmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This frequency should be normalised for better processing and it can be done by dividing the token's frequencies by the maximum frequency:"
      ],
      "metadata": {
        "id": "PeF7er2qgUJl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_freq = freq_words.most_common(1)[0][1]\n",
        "for word in freq_words.keys():\n",
        "  freq_words[word] = (freq_words[word]/max_freq)\n",
        "\n",
        "freq_words.most_common(5)"
      ],
      "metadata": {
        "id": "PrRJ5l2Aga-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we weigh each sentence based on the frequency of the keyword token present in each sentence. The result is stored as a key-value pair in `sent_strength` where keys are the sentences and the values are the weight of each sentence:"
      ],
      "metadata": {
        "id": "xQLm6730hYoQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sent_strength = {}\n",
        "for sent in doc.sents:\n",
        "  for word in sent:\n",
        "    if word.text in freq_words.keys():\n",
        "      if sent in sent_strength.keys():\n",
        "        sent_strength[sent] += freq_words[word.text]\n",
        "      else:\n",
        "        sent_strength[sent] = freq_words[word.text]\n",
        "\n",
        "print(sent_strength)"
      ],
      "metadata": {
        "id": "MYAwZi2yhwE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, the `nlargest` function is used to summarize the string. It takes 3 arguments:\n",
        "\n",
        "* Number of elements to extract\n",
        "* An Iterable (List/Tuple/Dictionary)\n",
        "* Condition to be satisfied\n",
        "\n",
        "This nlargest function returns a list containing the 3 sentences with the highest sentence strength score calculated in the previous step.\n",
        "\n",
        "We store this output in `summarized_sentences`:"
      ],
      "metadata": {
        "id": "cbmO3Ac-i3zQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summarized_sentences = nlargest(4, sent_strength, key=sent_strength.get)\n",
        "\n",
        "print(summarized_sentences)"
      ],
      "metadata": {
        "id": "YJ8N3d0oi3U1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lastly, convert the text data in the `summarized_sentences` to a string and print it:"
      ],
      "metadata": {
        "id": "bw3aCNVakFBr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_sentences = [w.text for w in summarized_sentences]\n",
        "summary = ' '.join(final_sentences)\n",
        "print(summary)"
      ],
      "metadata": {
        "id": "bCJEnu8IkEYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This example only shows a very limited application of [spaCy](https://spacy.io). The package has many powerful tools to create NLP applications."
      ],
      "metadata": {
        "id": "Qiro7L8QB_MT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Gradio\n",
        "[Gradio](https://gradio.app) is a fast way to demo your machine learning model with a nice web interface so that anyone can use it. The possibilities with Gradio are vast, this lab only scratches the surface.\n",
        "\n",
        "Here's the setup for a very basic UI:\n",
        "\n",
        "* First, define a function that does your main processing when users click the 'Submit' button in the UI.\n",
        "* Then define a gradio `Interface` called `demo`. This constructor has several arguments:\n",
        "  * The first in this example is the name of the function you defined\n",
        "  * The second is the type of inputs you want to capture (one text input in this case)\n",
        "  * The third is the type of output (also text)\n",
        "* Lastly, call the `Interface` object's `launch()` function to render the UI."
      ],
      "metadata": {
        "id": "Y5H2IZshEfqR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "id": "eMMGtUA1pCsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr"
      ],
      "metadata": {
        "id": "LaQ9oqy4USou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def greet(name):\n",
        "    return \"Hello \" + name + \"!\"\n",
        "\n",
        "demo = gr.Interface(fn=greet, inputs=\"text\", outputs=\"text\")\n",
        "\n",
        "demo.launch()"
      ],
      "metadata": {
        "id": "9cB8ln93SXvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can do any kind of processing inside your main function, like call other functions. Let's create a quick summarization tool using [sumy](https://github.com/miso-belica/sumy).\n",
        "\n",
        "* First, install the necessary packages for sumy.\n",
        "* Then import the modules for the summarization task."
      ],
      "metadata": {
        "id": "mCn4OS4UTx5t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sumy"
      ],
      "metadata": {
        "id": "GXYf9n28hTba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.lsa import LsaSummarizer as Summarizer\n",
        "from sumy.nlp.stemmers import Stemmer\n",
        "from sumy.utils import get_stop_words\n",
        "\n",
        "LANGUAGE = \"english\"\n",
        "\n",
        "def sumy_summarize(txt, n_sent=1):\n",
        "  parser = PlaintextParser.from_string(txt, Tokenizer(LANGUAGE))\n",
        "  stemmer = Stemmer(LANGUAGE)\n",
        "\n",
        "  summarizer = Summarizer(stemmer)\n",
        "  summarizer.stop_words = get_stop_words(LANGUAGE)\n",
        "\n",
        "  sents = \"\"\n",
        "  for sentence in summarizer(parser.document, n_sent):\n",
        "    sents += str(sentence) + \"\\n\"\n",
        "\n",
        "  return sents"
      ],
      "metadata": {
        "id": "-fugZc-eMGNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Return the output of the `sumy_summarize` function inside your main Gradio function, passing it the user input.\n",
        "\n",
        "Try using the text from the previous summarization example as input and experiment with the `n_sent` parameter."
      ],
      "metadata": {
        "id": "yFjvWx5yXiCy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sum(text):\n",
        "    return sumy_summarize(text)\n",
        "\n",
        "sum_demo = gr.Interface(fn=sum, inputs=\"text\", outputs=\"text\")\n",
        "sum_demo.launch()"
      ],
      "metadata": {
        "id": "70MRhhS0Twwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `sumy_summarize` function takes another parameter in addition to the text:\n",
        "\n",
        "* n_sent (int or None, optional) ‚Äì The number of sentences of the original text to be chosen for the summary.\n",
        "\n",
        "Let's add more input elements to the interface:\n",
        "\n",
        "* A checkbox widget allowing the user to capitalize the output\n",
        "* A text box to set the `n_sent`"
      ],
      "metadata": {
        "id": "qrkJephp0VLf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sum(text, make_caps, number_of_sentences):\n",
        "    summary = sumy_summarize(text, n_sent=number_of_sentences)\n",
        "    return summary.upper() if make_caps else summary\n",
        "\n",
        "sum_demo = gr.Interface(\n",
        "    fn=sum,\n",
        "    inputs=[\"text\", \"checkbox\", \"number\"],\n",
        "    outputs=\"text\"\n",
        ")\n",
        "\n",
        "sum_demo.launch()"
      ],
      "metadata": {
        "id": "4PR6dAf307bQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `Interface` is highly customizable. For example, you can give each element different labels, have placeholder text, change colors, etc. Check the [Gradio documentation](https://gradio.app/docs/) for details."
      ],
      "metadata": {
        "id": "f0MFhGObX37G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hugging Face\n",
        "\n",
        "\n",
        "ü§ó [Transformers](https://huggingface.co/docs/transformers/index) is a state-of-the-art Machine Learning for PyTorch, TensorFlow, and JAX, and provides APIs and tools to easily download and train state-of-the-art pretrained models.\n",
        "\n",
        "Begin by installing the Hugging Face `transformers` library:"
      ],
      "metadata": {
        "id": "alS1iTCWTMWx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "8hXjUCHttVIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the necessary modules for the tasks up ahead:"
      ],
      "metadata": {
        "id": "pRWI1tV_qC1J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoModelForSeq2SeqLM, AutoTokenizer, pipeline"
      ],
      "metadata": {
        "id": "q-3q00aZA10p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hugging Face pipelines make it simple to use any model for inference on any language, computer vision, speech, and multimodal tasks (see [docs](https://huggingface.co/docs/transformers/pipeline_tutorial)). The `pipeline()` automatically loads a default model and a preprocessing class capable of inference for your task.\n",
        "\n",
        "The following is an example of using a Hugging Face pipeline to do automatic abstractive summarization:\n",
        "\n",
        "* First create a pipeline object, here called `summarizer`\n",
        "* The `pipeline()` constructor takes two arguments:\n",
        "  * The name of the task (see [docs for existing pipelines](https://huggingface.co/docs/transformers/main_classes/pipelines)) - In our case, the task is 'summarization'\n",
        "  * The model - We can use [facebook/bart-large-xsum](https://huggingface.co/facebook/bart-large-xsum). This can be changed to any number of models available for this task on Hugging Face.\n",
        "\n",
        "* Next we define a string variable containing the text we want to summarize. Here it's called `text_to_summarize`.\n",
        "* Then call the `summarizer` pipeline, pass the string, and optionally set the max and min length, and method for generation\n",
        "* Finally, print the output"
      ],
      "metadata": {
        "id": "fF918IkJu-Zq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-xsum\")"
      ],
      "metadata": {
        "id": "VWCSlBv73xdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_to_summarize = \"\"\" New York (CNN)When Liana Barrientos was 23 years old, she got married in Westchester County, New York.\n",
        "A year later, she got married again in Westchester County, but to a different man and without divorcing her first husband.\n",
        "Only 18 days after that marriage, she got hitched yet again. Then, Barrientos declared \"I do\" five more times, sometimes only within two weeks of each other.\n",
        "In 2010, she married once more, this time in the Bronx. In an application for a marriage license, she stated it was her \"first and only\" marriage.\n",
        "Barrientos, now 39, is facing two criminal counts of \"offering a false instrument for filing in the first degree,\" referring to her false statements on the\n",
        "2010 marriage license application, according to court documents.\n",
        "Prosecutors said the marriages were part of an immigration scam.\n",
        "On Friday, she pleaded not guilty at State Supreme Court in the Bronx, according to her attorney, Christopher Wright, who declined to comment further.\n",
        "After leaving court, Barrientos was arrested and charged with theft of service and criminal trespass for allegedly sneaking into the New York subway through an emergency exit, said Detective\n",
        "Annette Markowski, a police spokeswoman. In total, Barrientos has been married 10 times, with nine of her marriages occurring between 1999 and 2002.\n",
        "All occurred either in Westchester County, Long Island, New Jersey or the Bronx. She is believed to still be married to four men, and at one time, she was married to eight men at once, prosecutors say.\n",
        "Prosecutors said the immigration scam involved some of her husbands, who filed for permanent residence status shortly after the marriages.\n",
        "Any divorces happened only after such filings were approved. It was unclear whether any of the men will be prosecuted.\n",
        "The case was referred to the Bronx District Attorney\\'s Office by Immigration and Customs Enforcement and the Department of Homeland Security\\'s\n",
        "Investigation Division. Seven of the men are from so-called \"red-flagged\" countries, including Egypt, Turkey, Georgia, Pakistan and Mali.\n",
        "Her eighth husband, Rashid Rajput, was deported in 2006 to his native Pakistan after an investigation by the Joint Terrorism Task Force.\n",
        "If convicted, Barrientos faces up to four years in prison.  Her next court appearance is scheduled for May 18.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "cKSankLlaTsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary = summarizer(text_to_summarize, max_length=130, min_length=30, do_sample=False)\n",
        "print(summary)"
      ],
      "metadata": {
        "id": "suj-aGanxeV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take things up a notch and create a non-goal-oriented chatbot using pipelines and Gradio.\n",
        "\n",
        "We can define our model and tokenizer explicitly to choose the model best suited for the task, instead of relying on the pipeline's default model. A good model for aimless chit-chat is the [Blenderbot](https://huggingface.co/facebook/blenderbot-400M-distill) model. Note that there are many other chat models you could choose for this task.\n",
        "\n",
        "Start by defining the tokenizer and model:"
      ],
      "metadata": {
        "id": "A-HWgZ51qHkI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat_tokenizer = AutoTokenizer.from_pretrained(\"facebook/blenderbot-400M-distill\")\n",
        "chat_model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/blenderbot-400M-distill\")"
      ],
      "metadata": {
        "id": "UvXhr12zp29P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we create a text generation pipeline called `bot` using the model and tokenizer we defined above:"
      ],
      "metadata": {
        "id": "HjngaV67jOF-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bot = pipeline(task=\"text2text-generation\", model=chat_model, tokenizer=chat_tokenizer, device=\"cuda\")"
      ],
      "metadata": {
        "id": "-ekGGlgXGdK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to build a basic chatbot interface using a pretrained model, we take the following steps:\n",
        "\n",
        "* Initialize the `chat_history` object outside the function, in order to keep building the dialog history over time, so the model has the dialog context when it generates responses.\n",
        "* Define a `chat` function that takes text from user input and keeps a history of the dialog across turns. This function:\n",
        "  * Adds the user's current input to the `chat_history`.\n",
        "  * Passes the `chat_history` object to the `bot` in the form of a string to make the model generate a response.\n",
        "  * Get the most recent response from the `bot`object.\n",
        "  * Then we update the `history`, which is a list of tuples: (user-input, bot-response)\n",
        "* Lastly, we define a Gradio `Interface` that:\n",
        "  * Calls the `chat` function\n",
        "  * Takes text as input\n",
        "  * Creates widgets from a predefined Gradio layout for a 'chatbot'\n",
        "  * Retains a 'state' of the dialog by retaining a list of the chat `history`.\n"
      ],
      "metadata": {
        "id": "JccTsNDJj55M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history = []\n",
        "def chat(input, history=[]):\n",
        "    chat_history.append('User: '+ input)\n",
        "    output = bot.predict(\", \".join(chat_history))[0]['generated_text']\n",
        "    history.append('User: '+ input)\n",
        "    history.append('Chatbot: '+ output)\n",
        "    response = [(history[i], history[i+1]) for i in range(0, len(history)-1, 2)]\n",
        "    return response, history\n",
        "\n",
        "chat_interface = gr.Interface(\n",
        "    fn=chat,\n",
        "    theme=\"default\",\n",
        "    css=\".footer {display:none !important}\",\n",
        "    inputs=[\"text\", \"state\"],\n",
        "    outputs=[\"chatbot\", \"state\"],\n",
        ")\n",
        "chat_interface.launch()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "w0-RczVOB3jA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You now have a variety of powerful tools at your disposal to rapidly create interesting and useful NLP applications!"
      ],
      "metadata": {
        "id": "FIb5zlT6zB7h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment\n",
        "Complete the following questions and hand in your solution in Canvas before 23:59 Friday October 11th. Remember to save your file before uploading it."
      ],
      "metadata": {
        "id": "76KqmaBLWCh6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1\n",
        "Compare the outputs of the three summarization methods covered in this notebook.\n",
        "\n",
        "Use the same piece of text for each method to answer these questions:\n",
        "\n",
        "1. Which method performs the best, in your opinion?\n",
        "2. What are the pros and cons of each method?\n",
        "3. What kind of summarization is each method doing?"
      ],
      "metadata": {
        "id": "vZA9xn7BFl27"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your solution here\n"
      ],
      "metadata": {
        "id": "m2GikbshFpsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2\n",
        "Create a sentiment classifier using Gradio and Huggingface.\n",
        "\n",
        "* Augment the simple version of the Gradio interface.\n",
        "* Add a Huggingface 'text-classification' pipleline, using the 'cardiffnlp/twitter-roberta-base-sentiment' model.\n",
        "\n",
        "This model outputs a list that contains one dictionary object. In this dictionary, the predicted class is the value of the key 'label'. The model outputs one of three sentiment classes:\n",
        "\n",
        "* `LABEL_0` for negative\n",
        "* `LABEL_1` for neutral\n",
        "* `LABEL_2` for positive\n",
        "\n",
        "Your app should take some text as input and output **one** of these three words:\n",
        "\n",
        "* Positive\n",
        "* Neutral\n",
        "* Negative"
      ],
      "metadata": {
        "id": "WDxnK0GhgUto"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Alter this codeblock and/or create as many blocks as necessary to accomplish the task for this part\n",
        "\n",
        "def analyze(text):\n",
        "    return \"Echo: \" + text\n",
        "\n",
        "sentiment_analyzer = gr.Interface(\n",
        "    fn=analyze,\n",
        "    inputs=\"text\",\n",
        "    outputs=\"text\"\n",
        ")\n",
        "\n",
        "sentiment_analyzer.launch()"
      ],
      "metadata": {
        "id": "9OtdSpNct2eo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3\n",
        "\n",
        "Create a chatbot that can do these three things:\n",
        "\n",
        "*   Summarize text\n",
        "*   Analyze sentiment\n",
        "*   Mindless chit-chat\n",
        "\n",
        "Augment the chatbot code below to accomplish the following:\n",
        "\n",
        "1. When the user ticks a checkbox that says 'do_summary':\n",
        "  * The user's long text input should be summarized by the bot using the pipeline defined earlier and responds with the summary\n",
        "  * This process may bypass the default chit-chat functionality\n",
        "2. When the user ticks a checkbox that says 'do_sentiment':\n",
        "  * The user's text input may be analyzed by the bot for sentiment using the method created in Part 2 and responds accordingly\n",
        "  * This process may bypass the default chit-chat functionality\n",
        "3. Process any other input using the default chat functionality"
      ],
      "metadata": {
        "id": "WBke_7MfuceR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Alter this codeblock and/or create as many blocks as necessary to accomplish the task for this part\n",
        "\n",
        "bot = pipeline(task=\"text2text-generation\", model=chat_model, tokenizer=chat_tokenizer, device=\"cuda\")\n",
        "\n",
        "chat_history = []\n",
        "\n",
        "def chat(input, history=[]):\n",
        "    chat_history.append('User: '+ input)\n",
        "    output = bot.predict(\", \".join(chat_history))[0]['generated_text']\n",
        "    history.append('User: '+ input)\n",
        "    history.append('Chatbot: '+ output)\n",
        "    response = [(history[i], history[i+1]) for i in range(0, len(history)-1, 2)]\n",
        "    return response, history\n",
        "\n",
        "chat_interface = gr.Interface(\n",
        "    fn=chat,\n",
        "    theme=\"default\",\n",
        "    css=\".footer {display:none !important}\",\n",
        "    inputs=[\"text\", \"state\"],\n",
        "    outputs=[\"chatbot\", \"state\"],\n",
        ")\n",
        "chat_interface.launch()"
      ],
      "metadata": {
        "id": "FE54O0p6f8YI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}