{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/giorgiosld/Natural-Language-Processing/blob/main/lab2/T_725_Lab02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pajD7caf9pX7"
      },
      "source": [
        "# T-725 Natural Language Processing: Lab 2\n",
        "In today's lab, we will be working with text classification.\n",
        "\n",
        "To begin with, do the following:\n",
        "* Select `\"File\" > \"Save a copy in Drive\"` to create a local copy of this notebook that you can edit.\n",
        "* Select `\"Runtime\" > \"Run all\"` to run the code in this notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsSxs1XKhi_4"
      },
      "source": [
        "## List comprehensions in Python\n",
        "List comprehensions are a concise way of creating lists in Python, and take the form:\n",
        "\n",
        "```python\n",
        "[expression for item in iterable]\n",
        "```\n",
        "\n",
        "A list comprehension creates a new list by evaluating some expression for every item in a given iterable (such as a string, a list or a dictionary). Let's look at an example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxB1Ip77hzFL"
      },
      "source": [
        "sentence = \"In a hole in the ground there lived a hobbit.\"\n",
        "words = sentence.split()\n",
        "print(words)\n",
        "\n",
        "# Example of a list comprehension\n",
        "word_lengths = [len(word) for word in words]\n",
        "print(word_lengths)\n",
        "\n",
        "# This is equal to\n",
        "word_lengths = []\n",
        "for word in words:\n",
        "  word_lengths.append(len(word))\n",
        "\n",
        "print(word_lengths)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWw6IRBgkQbc"
      },
      "source": [
        "You can also add a conditional statement to list comprehensions, so that the expression will only be evaluated for items that meet a certain criteria:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSRG8kBdjZKA"
      },
      "source": [
        "e_words = [word for word in words if len(word) > 5]\n",
        "print(e_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtTTJDJ5qZML"
      },
      "source": [
        "Python also has set and dictionary comprehensions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jniqHWPoqd8E"
      },
      "source": [
        "lowercase_characters = {c.lower() for c in sentence}\n",
        "print(lowercase_characters)\n",
        "\n",
        "word_length = {word: len(word) for word in words}\n",
        "print(word_length['ground'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vY3Y_4kIbpaU"
      },
      "source": [
        "A nested list is a list within another list. You can iterate through nested lists in the following way:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AW8w46PbuFM"
      },
      "source": [
        "# A list of countries and their capitals within different continents\n",
        "continents = [\n",
        "    [('Iceland', 'Reykjav√≠k'), ('Germany', 'Berlin'), ('Spain', 'Madrid')],  # Europe\n",
        "    [('Japan', 'Tokyo'), ('China', 'Beijing'), ('South Korea', 'Seoul')],  # Asia\n",
        "    [('Nigeria', 'Abuja'), ('Algeria', 'Algiers'), ('Angola', 'Luanda')]  # Africa\n",
        "]\n",
        "\n",
        "# Create a list of all the countries in the previous list\n",
        "[country for continent in continents for (country, capital) in continent]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBrOT1vNcYZ0"
      },
      "source": [
        "## Sentiment analysis with NLTK\n",
        "[Chapter 6](https://www.nltk.org/book/ch06.html) of the NLTK book shows how the toolkit can be used to create document classifiers, including a sentiment analyzer. The NLTK includes the `movie_reviews` corpus, which contains 2,000 movie reviews. Half of the reviews have been labelled as **positive** and the other half as **negative**. Let's download it and take a look:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xx01nR8x9qgq"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import movie_reviews\n",
        "nltk.download('punkt')\n",
        "\n",
        "nltk.download('movie_reviews')\n",
        "print(\"Categories:\", movie_reviews.categories())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AppRxScjc9vx"
      },
      "source": [
        "As expected, there are two categories: `pos` for positive reviews and `neg` for negative reviews. For this particular corpus, each review is stored as a separate text file. To get a list of all the text files in the corpus, we can use `movie_reviews.fileids()`. We can also get a list of files for a specific category:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GI8EjJoo_1jz"
      },
      "source": [
        "pos_fileids = movie_reviews.fileids('pos')\n",
        "neg_fileids = movie_reviews.fileids('neg')\n",
        "\n",
        "print(pos_fileids[:5])  # The first 5 positive reviews\n",
        "print(neg_fileids[:5])  # The first 5 negative reviews"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQ_C9_80AEDo"
      },
      "source": [
        "We can get a list of all the tokens in the corpus with `movie_reviews.words()`. We can also specify a filename to get a single tokenized review:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZ8mPNPGcnbN"
      },
      "source": [
        "pos_reviews = [movie_reviews.words(fid) for fid in pos_fileids]\n",
        "neg_reviews = [movie_reviews.words(fid) for fid in neg_fileids]\n",
        "\n",
        "print(pos_reviews[0][:10])  # The first 10 tokens of the first positive review\n",
        "print(neg_reviews[0][:10])  # The first 10 tokens of the first negative review"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g04vosHorN6F"
      },
      "source": [
        "Some words, such as *brilliant* and *memorable*, are more strongly associated with positive reviews than negative ones. Similarly, *boring* and *unfunny* have a stronger association with negative reviews.\n",
        "\n",
        "Using the movie review corpus, we can train a classifier to predict whether a given review is positive or negative. The classifier extracts a set of *features* from every review, which are then used to make the classification. In this case, the features we use will be a dictionary that tells us whether each of the 2,000 most common words in the corpus is present within a review or not."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeixOJ-lu1fD"
      },
      "source": [
        "# Create a set with 2,000 of the most frequent words in the movie review corpus\n",
        "movie_fd = nltk.FreqDist(movie_reviews.words())\n",
        "movie_words = {word for word, count in movie_fd.most_common(2000)}\n",
        "\n",
        "# For a given review (in the form of a list or set of tokens), create a\n",
        "# dictionary which tells us which words are present and which are not.\n",
        "def get_review_features(review):\n",
        "  review_words = set(review)\n",
        "  return {word: word in review_words for word in movie_words}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-r6kXma8-eB"
      },
      "source": [
        "# Let's see how this works for the first positive review:\n",
        "example_features = get_review_features(pos_reviews[0])\n",
        "print(\"'funny' is in the review:\", example_features['funny'])\n",
        "print(\"'boring' is in the review:\", example_features['boring'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KK5rLkC7TCVC"
      },
      "source": [
        "Next, let's create a training set that we can use to train a Naive Bayesian classifier. The training set, in this case, is a list of tuples in the format `[(features, category), ...]`, where `features` is a dictionary from `get_review_features()` and `category` is either `pos` or `neg`, depending on whether the review is positive or negative. To get an idea of how well the classifier performs, we're going to reserve 10% of the reviews for testing. That means that we'll be training our classifier on 1800 examples and testing it on 200 examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqhiywiwrhxW"
      },
      "source": [
        "pos_examples = [(get_review_features(review), 'pos') for review in pos_reviews]\n",
        "neg_examples = [(get_review_features(review), 'neg') for review in neg_reviews]\n",
        "\n",
        "movie_training = pos_examples[:900] + neg_examples[:900]  # 1800 examples total\n",
        "movie_test = pos_examples[900:] + neg_examples[900:]  # 200 examples total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jfty2sigVuf3"
      },
      "source": [
        "Now we have everything we need to train our classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAqq1Z72wLHl"
      },
      "source": [
        "movie_classifier = nltk.NaiveBayesClassifier.train(movie_training)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3inpPaRDV0_6"
      },
      "source": [
        "How well does it perform on the test set?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7eJMkqdw7Eb"
      },
      "source": [
        "print(\"Accuracy:\", nltk.classify.accuracy(movie_classifier, movie_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxakIFH5WNH_"
      },
      "source": [
        "The classifier achieves an accuracy of 81.5%. Let's take a look at which words have the biggest weights:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibMPC0QCxefo"
      },
      "source": [
        "movie_classifier.show_most_informative_features(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPSLZUxt9uMY"
      },
      "source": [
        "# Assignment\n",
        "Answer the following questions and hand in your solution in Canvas before 23:59 on September 6th. Remember to save your file before uploading it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQN5Qq9z97Yc"
      },
      "source": [
        "## Question 1\n",
        "The NLTK also includes a `subjectivity` corpus, which contains a collection of sentences that have either been categorized as **subjective** (emotional, expressing personal feelings and views)  or **objective** (more rational, factual). Some examples:\n",
        "\n",
        "* **Objective sentences**:\n",
        "  * uma thurman stars in quentin tarantino's fourth film venture , kill bill .  \n",
        "  * he lives in a motor garage with his six friends .\n",
        "  * the ensuing battle was one of the most savage in u . s . history .\n",
        "* **Subjective sentences**:\n",
        "  * seagal's strenuous attempt at a change in expression could very well clinch him this year's razzie .\n",
        "  * de niro cries . you'll cry for your money back .\n",
        "  * a heroic tale of persistence that is sure to win viewers' hearts .\n",
        "\n",
        "Unlike the movie review corpus, where every review is stored in separate file, here there is only one file for each category.\n",
        "\n",
        "Complete the following tasks:\n",
        "1. Import and download the `subjectivity` corpus.\n",
        "2. Find the names of each category.\n",
        "3. Using the category names, get the relative path of each file.\n",
        "4. Get a list of tokenized sentences for each category (using `subjectivity.sents(fileid)`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zc1EM7TvnYaC"
      },
      "source": [
        "# Your solution here\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaXJ-5Lr97_w"
      },
      "source": [
        "## Question 2\n",
        "Complete the following tasks:\n",
        "1. Create a set with the 2,000 most common words in the `subjectivity` corpus using `nltk.FreqDist()`.\n",
        "2. Create a function that takes a single, tokenized sentence as input (e.g., `['the', 'ensuing', 'battle', ...]`), and returns a dictionary of the 2,000 most frequent words and whether or not they are in the sentence (e.g., `{'battle': True, 'amusing': False, ...}`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Bx4kZSW98xq"
      },
      "source": [
        "# Your solution here\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vrxhu0iB99YN"
      },
      "source": [
        "## Question 3\n",
        "Complete the following tasks:\n",
        "1. Create a training set with 9,000 sentences (4,500 of each category)\n",
        "2. Create a test set with 1,000 sentences (500 of each category)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08xWq_S899xK"
      },
      "source": [
        "# Your solution here\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HToPuGzX9-Cl"
      },
      "source": [
        "## Question 4\n",
        "Complete the following tasks:\n",
        "1. Train a Naive Bayes classifier using the training set from the previous question.\n",
        "2. Evaluate the classifier on the test set. How accurate is it?\n",
        "3. Find the 20 most informative features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHrN1tvKZa4e"
      },
      "source": [
        "# Your solution here\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 5\n",
        "Dialog acts are sort of the type of *action* performed by the speaker. In the instant messaging corpus dataset 'NPS', each utterance is labeled with one of 15 dialogue act types, such as **Statement**, **Emotion**, **ynQuestion**, **Continuer**, etc.\n",
        "\n",
        "Your task is to classify text from the NPS corpus into two dialog acts: **whQuestion** or **Emotion**.\n",
        "\n",
        "Start by downloading the NPS corpus and getting all posts from the corpus:"
      ],
      "metadata": {
        "id": "f-J9BEEQFrbo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('nps_chat')\n",
        "posts = nltk.corpus.nps_chat.xml_posts()"
      ],
      "metadata": {
        "id": "Qtc_pG1qJZgh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a list that only includes posts of class **Emotion** and **whQuestion**. You can access the class of a post by calling `post.get(\"class\")`."
      ],
      "metadata": {
        "id": "p0wQn19UqfKV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your solution here\n"
      ],
      "metadata": {
        "id": "H316uXLTrRUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Randomize the posts and create a training set and a test set, where the first 1300 **Emotion + whQuestion** posts are used for training and the rest for testing."
      ],
      "metadata": {
        "id": "t1K-jkXCzv4T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your solution here\n"
      ],
      "metadata": {
        "id": "1SQdx2rY0LG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a list of the 200 most frequent tokens in the training set. You can access the text of a `post` object by calling `post.text`. Remember that the **split** function will use whitespace to tokenize a string: `some_string.split()`"
      ],
      "metadata": {
        "id": "38Klc5bHxDvW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your solution here\n"
      ],
      "metadata": {
        "id": "cmBZL8wxso-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Define two feature selection functions that take a string as input and output a dictionary of features:\n",
        "* `get_word_features(string)`\n",
        "* `get_custom_features(string)`\n",
        "\n",
        "Begin by defining `get_word_features`. This function should use the words as features, just like in the movie review example above.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tyC-0es9KwTq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your solution here\n"
      ],
      "metadata": {
        "id": "6b6mKRLiFqEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, define `get_custom_features`. This function should extract the features from the text that characterize the **Emotion** and **whQuestions** classes."
      ],
      "metadata": {
        "id": "n4LmO-UaJCHj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your solution here\n"
      ],
      "metadata": {
        "id": "w9CBUttQKEzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conduct the following tasks:\n",
        "*   Train two Naive Bayes classifiers on the **Emotion + whQuestions** training set: one that uses the `get_word_features` function and another using `get_custom_features`.\n",
        "*   Evaluate each classifier on the test set. How accurate are they? Which one is better?\n",
        "*   What are the 20 most informative features for each classifier?\n"
      ],
      "metadata": {
        "id": "KxGa5GS1J3aG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your solution here\n"
      ],
      "metadata": {
        "id": "15ERz1hnK8at"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}