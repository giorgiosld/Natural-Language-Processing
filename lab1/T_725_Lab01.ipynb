{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/giorgiosld/Natural-Language-Processing/blob/main/lab1/T_725_Lab01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "cA4f3BP_gZUC"
      },
      "source": [
        "# T-725 Natural Language Processing: Lab 1\n",
        "In these labs, we will be using the [Python 3](https://www.python.org/) programming language and the [Natural Language Toolkit (NLTK)](https://www.nltk.org/). We will also be using Google Colab, a free service hosted by Google, which gives us access to a Linux machine that comes pre-installed with Python 3 and the NLTK."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMY8d4CJ46K2"
      },
      "source": [
        "## Using Google Colab\n",
        "Google Colab allows users to work with \"notebooks\", which consists of text cells and code cells. Text cells can be edited by double clicking them. A code cell can be executed by selecting it and pressing `Ctrl + Enter`. Code is shared between cells, meaning that you can, for example, create a variable in one cell and use it in another cell later on.\n",
        "\n",
        "To begin with, do the following:\n",
        "* Select `\"File\" > \"Save a copy in Drive\"` to create a local copy of this notebook that you can edit.\n",
        "* Select `\"Runtime\" > \"Run all\"` to run all of the code cells in the notebook.\n",
        "\n",
        "## Resources\n",
        "* [The Python Standard Library](https://docs.python.org/3/library/index.html) - an overview of the built-in libraries in Python with a lot of examples.\n",
        "* [The Python Tutorial](https://docs.python.org/3/tutorial/index.html) - an official tutorial that gives a brief overview of the language.\n",
        "* [Automate the Boring Stuff with Python](https://automatetheboringstuff.com/) - a free book that offers a good introduction to the Python programming language to beginners.\n",
        "* [Natural Language Processing with Python](http://www.nltk.org/book/) - a free companion book to the NLTK toolkit.\n",
        "\n",
        "## Setting Python and the NLTK up on your own machine\n",
        "* [Python 3](https://realpython.com/installing-python/) - installation instructions\n",
        "* [NLTK](https://www.nltk.org/install.html) - installation instructions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f19uyslsaYw4"
      },
      "source": [
        "## String methods in Python\n",
        "There are many ways to manipulate strings in Python. A full list of methods for the String class may be found in the [library reference](https://docs.python.org/3/library/stdtypes.html#string-methods)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzTEm3RCgZUU"
      },
      "source": [
        "a_string = \"It was the best of times, it was the worst of times\"\n",
        "\n",
        "print(\"Lowercase:\", a_string.lower())\n",
        "print(\"'times' count:\", a_string.count('times'))\n",
        "print(\"First occurence of 'best':\", a_string.find('best'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmTHYN-agZVa"
      },
      "source": [
        "## Lists, sets and built-in functions\n",
        "Lists and sets are two kinds of collections that can be used in Python.\n",
        "* A **list** is an *ordered* sequence of elements. Lists are enclosed with square brackets, and the elements are separated with a comma (e.g., `a_list = [\"This\", \"is\", \"a\", \"list\"]`).\n",
        "* A **set** is a collection of *unordered* and *unique* elements (meaning that it contains no duplicates). Sets are enclosed by curly braces, and the elements are separated by commas (e.g., `a_set = {\"This\", \"is\", \"a\", \"set\"}`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "8NWJM2rGgZVf"
      },
      "source": [
        "# Variables can be converted to lists and sets with the list() and set() functions\n",
        "char_list = list(a_string)\n",
        "char_set = set(a_string)\n",
        "\n",
        "# You can also split strings on certain characters to create a list of strings\n",
        "words = a_string.split()  # Splits on whitespaces by default\n",
        "print(\"Split string:\", words)\n",
        "\n",
        "# The len() and max() built-in methods\n",
        "print(\"Unique characters:\", char_set)\n",
        "print(\"No. of unique characters:\", len(char_set))\n",
        "print(\"Longest word:\", max(words, key=len))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUt68ulEpfuw"
      },
      "source": [
        "You can use the built-in function `help()` to quickly access documentation for a given object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "En1YvSC4qHJ8"
      },
      "source": [
        "help(max)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9X_TWoEUgZVw"
      },
      "source": [
        "## Indices and slicing\n",
        "You can get characters and substrings at specific indexes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_p4o5Lc_gZV2"
      },
      "source": [
        "print(\"String:\", a_string)\n",
        "print(\"First character:\", a_string[0])  # Indices in Python start at 0\n",
        "print(\"Last character:\", a_string[-1])  # A negative index starts counting from the end\n",
        "\n",
        "# We can get ranges of elements by slicing a list\n",
        "print(\"Characters 11 to 14:\", a_string[11:15])\n",
        "print(\"First 6 characters:\", a_string[:6])\n",
        "print(\"Last 5 characters:\", a_string[-5:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wQ9qvZ2gZV_"
      },
      "source": [
        "## The Natural Language Toolkit (NLTK)\n",
        "The NLTK book, [Natural Language Processing with Python](http://www.nltk.org/book/), is an introduction to natural language processing in Python, using the NLTK library. [Chapter 1](http://www.nltk.org/book/ch01.html) is relevant to this lab. The NLTK comes with a lot of data, such as corpora and trained models. We can download this data with the `nltk.download()` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "txGWuokGgZWF"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import gutenberg\n",
        "\n",
        "# Download the 'gutenberg' corpus, which is a collection of books in the public domain\n",
        "nltk.download('gutenberg')\n",
        "\n",
        "# Get a plain-text version of Moby Dick (contained in a single string)\n",
        "moby_raw = gutenberg.raw('melville-moby_dick.txt')\n",
        "\n",
        "# Get all the tokens in Moby Dick (as a list of strings)\n",
        "moby_tokens = gutenberg.words('melville-moby_dick.txt')\n",
        "\n",
        "# Print the first 10 tokens of Moby Dick\n",
        "print(\"First 10 tokens:\", moby_tokens[:10])\n",
        "\n",
        "# Print the first 250 characters of Moby Dick\n",
        "print(\"\\nFirst 250 characters:\\n\", moby_raw[:250])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bk7r17iXgZW-"
      },
      "source": [
        "NLTK includes a Text class for analyzing the contents of texts. Let's print a concordance for the word *Iceland*:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCPcW0p9gZXB"
      },
      "source": [
        "moby_text = nltk.Text(moby_tokens)\n",
        "moby_text.concordance('Iceland')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntDr5ACsgZXM"
      },
      "source": [
        "NTLK offers several ways to segment text into sentences and tokenize it. Let's see how `nltk.sent_tokenize()` and `nltk.word_tokenize()` work:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IZ1nLySgZXQ"
      },
      "source": [
        "# Download the Punkt tokenizer model\n",
        "nltk.download('punkt')\n",
        "\n",
        "moby_sentences = nltk.sent_tokenize(moby_raw)  # Split raw text into sentences\n",
        "tokens = nltk.word_tokenize(moby_sentences[3])  # Split a string into tokens\n",
        "\n",
        "print(\"First 5 sentences:\")\n",
        "for sentence in moby_sentences[:5]:\n",
        "    print(\">>>\", sentence)\n",
        "\n",
        "print(f\"\\nTotal number of sentences: {len(moby_sentences):,}\")\n",
        "\n",
        "print(\"\\nTokens:\", tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fC_OW-lUgZXo"
      },
      "source": [
        "## Regular expressions\n",
        "The Python standard library includes the `re` module for handling regular expressions ([reference](https://docs.python.org/3/library/re.html)). In Python, regular expression patterns should be created using *raw* strings, which are prefixed with an `r`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "I7cDMt90gZXs"
      },
      "source": [
        "import re\n",
        "re.findall(r'\\b\\S{9,}est\\b', moby_raw)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0z_417IcgZX1"
      },
      "source": [
        "You can capture text that matches a specific part of a pattern in a group by enclosing it within parentheses. When making substitutions with `re.sub()`, you can refer to these groups using `\\number` (e.g., `\\1` and `\\2`), where the number refers to their position in the pattern:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHyZVF5pgZX3"
      },
      "source": [
        "another_string = \"The grapes of wrath\"\n",
        "re.sub(r'(\\S+) of (\\S+)', r'\\2 of \\1', another_string)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-U35BXYgZYD"
      },
      "source": [
        "NLTK offers a simple way of searching for sequences of tokens using regular expressions, where tokens can be enclosed in angle brackets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "pRChhPklgZYH"
      },
      "source": [
        "# First we must create a Text object from a list of tokens\n",
        "moby_text = nltk.Text(moby_tokens)\n",
        "\n",
        "# Let's search for sequences of four tokens which all begin with the letter S\n",
        "moby_text.findall(r'<[Ss].*>{4,}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIt3E060gZYQ"
      },
      "source": [
        "You can use groups to target specific tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "3L3nrg2SgZYV"
      },
      "source": [
        "moby_text.findall(r'<[Aa]n?>(<.+>)<ship>')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ksz_n_cwgZYb"
      },
      "source": [
        "# Assignment\n",
        "Answer the following questions and hand in your solution in Canvas before midnight, August 30th. Make a copy of this notebook (`\"File\" > \"Save a copy in Drive\"`) and enter your solutions in the cells below each question. Remember to save your file before uploading it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEcJJbIUgZYi"
      },
      "source": [
        "### Question 1\n",
        "Get the raw text of `carroll-alice.txt` (Alice in Wonderland) from the Gutenberg corpus in NLTK and tokenize it using `nltk.word_tokenize()`.\n",
        "\n",
        "1. How many tokens does it contain in total?\n",
        "2. How many unique tokens (types) does it contain?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "dsA5lgp0gZYx"
      },
      "source": [
        "# Your solution here\n",
        "alice_raw = gutenberg.raw('carroll-alice.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alice_tokens = nltk.word_tokenize(alice_raw)"
      ],
      "metadata": {
        "id": "jEwXH3ZfQt01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(alice_tokens))"
      ],
      "metadata": {
        "id": "Ei_Odks3RRaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(set(alice_tokens)))"
      ],
      "metadata": {
        "id": "N7DTFTksRWB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5M9OMFNxgZY6"
      },
      "source": [
        "### Question 2\n",
        "Use `nltk.FreqDist()` to create a frequency distribution of all the tokens in Alice in Wonderland. What are the 20 most frequently occurring tokens?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "tOPA_dljgZY_"
      },
      "source": [
        "# Your solution here\n",
        "alice_fq = nltk.FreqDist(alice_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alice_fq.most_common(20)"
      ],
      "metadata": {
        "id": "UhLNGEreStFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ku3fBv6gZZH"
      },
      "source": [
        "### Question 3\n",
        "Use `nltk.sent_tokenize()` to segment Alice in Wonderland into sentences, then find the longest sentence in the book."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "-EuYbFqJgZZI"
      },
      "source": [
        "# Your solution here\n",
        "alice_sent = nltk.sent_tokenize(alice_raw)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(alice_sent)"
      ],
      "metadata": {
        "id": "LgS_K3YSTyKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.FreqDist(alice_sent)"
      ],
      "metadata": {
        "id": "fnx0qVKrT2gQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExlDi0jDgZZR"
      },
      "source": [
        "### Question 4\n",
        "Use a regular expression to find all tokens in Alice in Wonderland that contain an *x* and end with *ed*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtfpRxu5gZZT"
      },
      "source": [
        "# Your solution here\n",
        "set(re.findall(r'\\b\\S*x\\S*ed\\b', alice_raw))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0rG5i_VgZZa"
      },
      "source": [
        "### Bonus Question\n",
        "\n",
        "Use `re.sub()` to \"dehyphenate\" the following string:\n",
        "\n",
        ">It is a capital mistake to theo-  \n",
        ">rize before one has data. Insen-  \n",
        ">sibly one begins to twist facts  \n",
        ">to suit theories, instead of the-  \n",
        ">ories to suit facts.\n",
        "\n",
        "You will need to use groups to recombine the words. The resulting string should look like this:\n",
        "\n",
        ">It is a capital mistake to  \n",
        ">theorize before one has data.  \n",
        ">Insensibly one begins to twist facts  \n",
        ">to suit theories, instead of  \n",
        ">theories to suit facts.\n",
        "\n",
        "Remember that a \"newline\" character is represented by `\\n` in strings and regular expression patterns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "zKyWgOPkgZZe"
      },
      "source": [
        "# Your solution here\n",
        "hyphenated = \"\"\"\n",
        "It is a capital mistake to theo-\n",
        "rize before one has data. Insen-\n",
        "sibly one begins to twist facts\n",
        "to suit theories, instead of the-\n",
        "ories to suit facts.\n",
        "\"\"\"\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}