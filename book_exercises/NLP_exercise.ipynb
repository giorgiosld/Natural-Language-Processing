{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPLr1UqN1+B6uC8bOPhcg7t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/giorgiosld/Natural-Language-Processing/blob/main/book_exercises/NLP_exercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Computing with Language: Texts and Words\n"
      ],
      "metadata": {
        "id": "uW401WJzbiX0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Getting Started with NLTK\n",
        "Import NLTK and download NLTK Book Collection."
      ],
      "metadata": {
        "id": "N8G2YXE6_p-7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "ldB-JfSWLZGs",
        "outputId": "ff121f73-96d8-4438-f3d8-af4d59ace661",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-3b02390c7677>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/downloader.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(self, info_or_id, download_dir, quiet, force, prefix, halt_on_error, raise_on_error, print_error_to)\u001b[0m\n\u001b[1;32m    761\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdownload_dir\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_download_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interactive_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    764\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/downloader.py\u001b[0m in \u001b[0;36m_interactive_download\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1113\u001b[0m                 \u001b[0mDownloaderGUI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTclError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1115\u001b[0;31m                 \u001b[0mDownloaderShell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1116\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1117\u001b[0m             \u001b[0mDownloaderShell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/downloader.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1139\u001b[0m                 \u001b[0;34m\"q) Quit\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m             )\n\u001b[0;32m-> 1141\u001b[0;31m             \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Downloader> \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1142\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "See insight data."
      ],
      "metadata": {
        "id": "EBafLa3b_oA8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.book import *"
      ],
      "metadata": {
        "collapsed": true,
        "id": "3gBwne4sAEDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Search for concordance words inside the data."
      ],
      "metadata": {
        "id": "G4tQ0A2mCvKW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# print(text1)\n",
        "print(text1.concordance(\"monstrous\"))\n",
        "# print(text2.concordance(\"affection\"))\n",
        "# print(text2.concordance(\"lived\"))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "50Fxtv7bAfxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the concordance permits to see words in context is possible find out words appear in a similar rangd of contexts."
      ],
      "metadata": {
        "id": "MX94uvv2DgG6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text1.similar(\"monstrous\")\n",
        "text2.similar(\"monstrous\")"
      ],
      "metadata": {
        "id": "ism9lEMrDwTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It was seen that we get different results for different texts because it dependens on connotation. For this example *monstrous* has a positive connotation similar to the intensifier word *very*.\n",
        "But we can do more examining just the contexts shared by two or more words."
      ],
      "metadata": {
        "id": "aYTC9_hIEDrm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text2.common_contexts([\"monstrous\", \"very\"])"
      ],
      "metadata": {
        "id": "nrvzC9YAE5hH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also determine the *location* of a word in the text. Using a particular plot named **disperion plot**."
      ],
      "metadata": {
        "id": "7kPDRlK_Fyzm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text4.dispersion_plot([\"citizens\", \"democracy\", \"freedom\", \"duties\", \"America\"])"
      ],
      "metadata": {
        "id": "43f28HFwF8xS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Is also possible generate some random text in the various styles."
      ],
      "metadata": {
        "id": "fKoYNd0AGjv1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text3.generate()"
      ],
      "metadata": {
        "id": "klfLFq9MGqXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The concept of **token** is way to represent a sequence of charcters treated as a group. It is possible to see how many tokens are in a specific document."
      ],
      "metadata": {
        "id": "v-Us5DZvIqPd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(text3))\n",
        "print(sorted(set(text3)))\n",
        "print(len(set(text3)))"
      ],
      "metadata": {
        "id": "z8c-FxszJEqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Through the next commands it's possible to have an idea of the richness of text."
      ],
      "metadata": {
        "id": "RJFtxahhJXyJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(set(text3)) / len(text3)"
      ],
      "metadata": {
        "id": "WqbceVU0Jo9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Is also possible to see the number of occurencies and the percentage of the text is taken up by a specific word."
      ],
      "metadata": {
        "id": "K2tCc9CTJxPp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text3.count(\"smote\")\n",
        "100 * text4.count('a') / len(text4)"
      ],
      "metadata": {
        "id": "DVYPxtfbJ76i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definition of some functions helpful to measure the lexical diversity and its percentage in a text."
      ],
      "metadata": {
        "id": "78WAm731OH3x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lexical_diversity(text):\n",
        "  return len(set(text)) / len(text)\n",
        "\n",
        "def percentage(count, total):\n",
        "  return 100 * count / total"
      ],
      "metadata": {
        "id": "JGiuJ7EyN1KL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Frequency Distribtuions\n",
        "NLTK provide a way to see the frequency distribution, telling us the frequency of each vocabulary item in the text with the possibilty to plot it."
      ],
      "metadata": {
        "id": "k_VjURSCR_l6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fdist1 = FreqDist(text1)\n",
        "print(fdist1)\n",
        "fdist1.most_common(50)\n",
        "fdist1['whale']\n",
        "fdist1.plot(50, cumulative=True)"
      ],
      "metadata": {
        "id": "xHcCBZ2KSUqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If your goal is to identify the meaning of a particular text and the frequent words don't help up we can check for the ones that occure once only using the **hapaxes**."
      ],
      "metadata": {
        "id": "t6l0Rdm5T62M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fdist1.hapaxes()"
      ],
      "metadata": {
        "id": "02aenBQ-USlJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the purpose is to take a look for some words that comply a particular property it's possible use some notation from *set theory*:\n",
        "\n",
        "\n",
        "$\\{ w \\mid w \\in V \\ \\&\\ P(w) \\}$\n",
        "\n",
        "\n",
        "in Python the corresponding expression is given by:\n",
        "\n",
        "\n",
        "[w for w in V if p(w)]"
      ],
      "metadata": {
        "id": "ga-M5ID0U46Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "V = set(text1)\n",
        "long_words = [w for w in V if len(w) > 15]\n",
        "sorted(long_words)"
      ],
      "metadata": {
        "id": "BHMRUGpDW0BM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Following our goal (find words that characterize a text) the next task is to clean \"the noise\" given by hapaxes and *antiphilophists* (infrequent long words)."
      ],
      "metadata": {
        "id": "WN6f23tsXugV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fdist5 = FreqDist(text5)\n",
        "sorted(w for w in set(text5) if len(w) > 7 and fdist5[w] > 7)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEvEIGv8YNiA",
        "outputId": "94f860e9-c45f-42f5-9021-0a259f0feb12"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['#14-19teens',\n",
              " '#talkcity_adults',\n",
              " '((((((((((',\n",
              " '........',\n",
              " 'Question',\n",
              " 'actually',\n",
              " 'anything',\n",
              " 'computer',\n",
              " 'cute.-ass',\n",
              " 'everyone',\n",
              " 'football',\n",
              " 'innocent',\n",
              " 'listening',\n",
              " 'remember',\n",
              " 'seriously',\n",
              " 'something',\n",
              " 'together',\n",
              " 'tomorrow',\n",
              " 'watching']"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Collocations and Bigrams\n",
        "A **collocation** is a sequence of words that occur together unusually often. To get a handle on collocations, we start off by extracting from a text a list of word pairs, also known as **bigrams**."
      ],
      "metadata": {
        "id": "YU9Ak-pfavVd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list(bigrams([\"more\", \"is\", \"said\", \"than\", \"done\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UA8dqFFEcCDL",
        "outputId": "2a2e0f0c-8079-44ac-fc8a-78555a8aa0f2"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('more', 'is'), ('is', 'said'), ('said', 'than'), ('than', 'done')]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "According its definition, collocations are essentially just frequent bigrams. It's possible to find bigrams that occur more often based on frequency of the individual words."
      ],
      "metadata": {
        "id": "PL6cE2cFdmro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text4.collocations()\n",
        "text8.collocations()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0E8UHdUHd5Sb",
        "outputId": "93483a35-4d87-4a10-e6b5-c212fa7bf4d3"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "United States; fellow citizens; years ago; four years; Federal\n",
            "Government; General Government; American people; Vice President; God\n",
            "bless; Chief Justice; one another; fellow Americans; Old World;\n",
            "Almighty God; Fellow citizens; Chief Magistrate; every citizen; Indian\n",
            "tribes; public debt; foreign nations\n",
            "would like; medium build; social drinker; quiet nights; non smoker;\n",
            "long term; age open; Would like; easy going; financially secure; fun\n",
            "times; similar interests; Age open; weekends away; poss rship; well\n",
            "presented; never married; single mum; permanent relationship; slim\n",
            "build\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Counting Other Things\n",
        "To look at the distribution of word lengths in a text we start by deriving a list of the lengths of words, then counts the number of times each of these occurs. To see how frequent the different lengths of word are is possible do it using the following code."
      ],
      "metadata": {
        "id": "EIxFhe3feHAs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[len(w) for w in text1]\n",
        "fdist = FreqDist(len(w) for w in text1)\n",
        "print(fdist)\n",
        "fdist\n",
        "\n",
        "fdist.most_common()\n",
        "fdist.max()\n",
        "fdist[3]\n",
        "fdist.freq(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "R6_rSo1Eiv_y",
        "outputId": "8b556732-241b-4a87-d7a1-14e0d3c9ad38"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'text1' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d426c138bcb3>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFreqDist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfdist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'text1' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From this measure we can notice that the most frequent word length is 3 and appear for roughly 50000 (20%) of the words making up the book."
      ],
      "metadata": {
        "id": "p2bv6XYijRm2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SZQYvCUHmUN_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}